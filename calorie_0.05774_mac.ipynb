{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6071784-0fa1-434b-bc00-9e5b3213d082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1ecb503-04e3-4d75-919b-da15cd9e210c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750000, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Heart_Rate</th>\n",
       "      <th>Body_Temp</th>\n",
       "      <th>Calories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>36</td>\n",
       "      <td>189.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>64</td>\n",
       "      <td>163.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>39.7</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>51</td>\n",
       "      <td>161.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>39.8</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>20</td>\n",
       "      <td>192.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>40.7</td>\n",
       "      <td>140.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>female</td>\n",
       "      <td>38</td>\n",
       "      <td>166.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>40.6</td>\n",
       "      <td>146.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     Sex  Age  Height  Weight  Duration  Heart_Rate  Body_Temp  Calories\n",
       "0   0    male   36   189.0    82.0      26.0       101.0       41.0     150.0\n",
       "1   1  female   64   163.0    60.0       8.0        85.0       39.7      34.0\n",
       "2   2  female   51   161.0    64.0       7.0        84.0       39.8      29.0\n",
       "3   3    male   20   192.0    90.0      25.0       105.0       40.7     140.0\n",
       "4   4  female   38   166.0    61.0      25.0       102.0       40.6     146.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(\"/Users/nelllio/Desktop/Machine_Learning/playground-series/train.csv\")\n",
    "print(df.shape)\n",
    "df.head()\n",
    "#df ë³€ìˆ˜ ì§€ì • í›„ head()ë¥¼ í†µí•´ 5í–‰ê¹Œì§€ ë°ì´í„° í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35fdf56c-1a3d-470e-9364-e7cc14b23a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 750000 entries, 0 to 749999\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   id          750000 non-null  int64  \n",
      " 1   Sex         750000 non-null  object \n",
      " 2   Age         750000 non-null  int64  \n",
      " 3   Height      750000 non-null  float64\n",
      " 4   Weight      750000 non-null  float64\n",
      " 5   Duration    750000 non-null  float64\n",
      " 6   Heart_Rate  750000 non-null  float64\n",
      " 7   Body_Temp   750000 non-null  float64\n",
      " 8   Calories    750000 non-null  float64\n",
      "dtypes: float64(6), int64(2), object(1)\n",
      "memory usage: 51.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train = df.info()\n",
    "print(train)\n",
    "# countì— non-nullë¡œ ê²°ì¸¡ì¹˜ ì—†ëŠ” ê²ƒ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8cb45f5-2cd0-41dc-9509-072927198979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027790 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2141\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058849 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2143\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140724\n",
      "[LightGBM] [Info] Start training from score 4.141876\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065664 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2143\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045546 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2146\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.141163\n",
      "[LightGBM] [Info] Start training from score 4.141466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008363 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2144\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140493\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2141\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.142021\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016219 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2141\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.141786\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065705 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2144\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.141961\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.086400 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2140\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140974\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064179 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2143\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140854\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006751 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2140\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.141604\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032145 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2145\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.141523\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007056 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2143\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140854\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014914 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2142\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140429\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083022 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2146\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007462 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2141\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.142997\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024874 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2146\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.142381\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037550 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2144\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.142023\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041467 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2146\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.141589\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.084738 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2144\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.141844\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018730 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2141\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.141706\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032340 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2147\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.141103\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.139392 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2144\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140839\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008978 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2144\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140275\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067002 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2144\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140997\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038681 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2148\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140534\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011945 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2141\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.141052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013492 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2144\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140140\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026942 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2144\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.139282\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030385 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2144\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.139376\n",
      "ğŸ“Š 5-fold CV RMSLE: 0.0171 Â± 0.0001\n",
      "\n",
      "ğŸ“š ëª¨ë¸ í•™ìŠµ ì¤‘...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005977 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2144\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140559\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059197 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2144\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.077467 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2145\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.142413\n",
      "[LightGBM] [Info] Start training from score 4.140125\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059595 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2144\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.141428\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2146\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.141291\n",
      "ğŸ¯ Hold-out RMSLE: 0.0596\n",
      "\n",
      "ğŸš€ Kaggle ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\n",
      "ğŸ“š ì „ì²´ í›ˆë ¨ ë°ì´í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ ì¤‘...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2143\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140954\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031933 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2147\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140934\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034914 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2142\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033739 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2145\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039716 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2145\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.141530\n",
      "[LightGBM] [Info] Start training from score 4.142215\n",
      "[LightGBM] [Info] Start training from score 4.140089\n",
      "ğŸ”® í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...\n",
      "âœ… Submission íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: /Users/nelllio/Desktop/Machine_Learning/playground-series/submission.csv\n",
      "ğŸ“Š ì˜ˆì¸¡ëœ ì¹¼ë¡œë¦¬ ë²”ìœ„: 1.04 ~ 295.20\n",
      "ğŸ“Š í‰ê·  ì˜ˆì¸¡ ì¹¼ë¡œë¦¬: 88.18\n",
      "ğŸ“‹ Submission í˜•íƒœ:\n",
      "       id    Calories\n",
      "0  750000   27.653635\n",
      "1  750001  107.592008\n",
      "2  750002   87.117329\n",
      "3  750003  126.170766\n",
      "4  750004   76.208452\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 0. ë¼ì´ë¸ŒëŸ¬ë¦¬ (ë™ì¼)\n",
    "# ===============================\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "# ===============================\n",
    "# 1. ë°ì´í„° & FE (ë™ì¼)\n",
    "# ===============================\n",
    "\n",
    "def create_golden_features(df):\n",
    "    df = df.copy()\n",
    "    df['Duration_x_HeartRate']   = df['Duration'] * df['Heart_Rate']\n",
    "    df['Age_adjusted_HeartRate'] = df['Heart_Rate'] / (df['Age'] + 1)\n",
    "    df['BMI']                    = df['Weight'] / (df['Height'] / 100) ** 2\n",
    "    df['Metabolic_Intensity']    = df['Heart_Rate'] * df['Body_Temp']\n",
    "    df['Steps_per_min']          = df['Heart_Rate'] / (df['Duration'] / 60)\n",
    "    df['Weight_to_Age']          = df['Weight'] / (df['Age'] + 1)\n",
    "    df['HeartRate_to_BMI']       = df['Heart_Rate'] / (df['BMI'] + 1)\n",
    "    df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
    "    return df\n",
    "\n",
    "df = create_golden_features(df)\n",
    "features = [\n",
    "    'Duration','Heart_Rate','BMI','Body_Temp',\n",
    "    'Duration_x_HeartRate','Metabolic_Intensity',\n",
    "    'Age','Height','Weight',\n",
    "    'Steps_per_min','Age_adjusted_HeartRate',\n",
    "    'Weight_to_Age','HeartRate_to_BMI','Sex'\n",
    "]\n",
    "X, y = df[features], df['Calories']\n",
    "\n",
    "# ===============================\n",
    "# 2. RMSLE ìŠ¤ì½”ì–´ëŸ¬ + 5-fold CV\n",
    "# ===============================\n",
    "def rmsle(y_true, y_pred):\n",
    "    y_pred = np.maximum(y_pred, 0.1)                      # log ì•ˆì •í™”\n",
    "    return np.sqrt(mean_squared_error(np.log1p(y_true),\n",
    "                                      np.log1p(y_pred)))\n",
    "\n",
    "rmsle_scorer = make_scorer(rmsle, greater_is_better=False)  # ìŒìˆ˜ ë°˜í™˜(ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ===============================\n",
    "# 3. Base learner â€“ ë¯¸ì„¸ íŠœë‹\n",
    "#    (n_estimators â†‘, learning_rate â†“)\n",
    "# ===============================\n",
    "xgb = XGBRegressor(\n",
    "        n_estimators=500, learning_rate=0.05,\n",
    "        max_depth=6, subsample=0.9, colsample_bytree=0.9,\n",
    "        n_jobs=-1, random_state=42, verbosity=0)\n",
    "\n",
    "lgb = LGBMRegressor(\n",
    "        n_estimators=500, learning_rate=0.05,\n",
    "        max_depth=-1, subsample=0.9, colsample_bytree=0.9,\n",
    "        n_jobs=-1, random_state=42)\n",
    "\n",
    "cat = CatBoostRegressor(\n",
    "        n_estimators=500, learning_rate=0.05,\n",
    "        depth=8, l2_leaf_reg=3,\n",
    "        verbose=0, random_state=42)\n",
    "\n",
    "# ===============================\n",
    "# 4. ìŠ¤íƒœí‚¹ ì •ì˜ (ë™ì¼)\n",
    "# ===============================\n",
    "stack = StackingRegressor(\n",
    "    estimators=[('xgb', xgb), ('lgb', lgb), ('cat', cat)],\n",
    "    final_estimator=RidgeCV(),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 5-1. êµì°¨ê²€ì¦ RMSLE í™•ì¸\n",
    "# ===============================\n",
    "cv_scores = cross_val_score(stack, X, np.log1p(y),   # log1p ë³€í™˜ â‡ ë™ì¼ ê¸°ì¤€\n",
    "                            cv=kf, scoring=rmsle_scorer,\n",
    "                            n_jobs=-1)\n",
    "print(f\"ğŸ“Š 5-fold CV RMSLE: {(-cv_scores.mean()):.4f} Â± {cv_scores.std():.4f}\")\n",
    "\n",
    "# ===============================\n",
    "# 5-2. í™€ë“œì•„ì›ƒ í•™ìŠµ/í‰ê°€\n",
    "# ===============================\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42)\n",
    "print(\"\\nğŸ“š ëª¨ë¸ í•™ìŠµ ì¤‘...\")\n",
    "stack.fit(X_train, np.log1p(y_train))\n",
    "y_pred = np.expm1(stack.predict(X_val))\n",
    "print(f\"ğŸ¯ Hold-out RMSLE: {rmsle(y_val, y_pred):.4f}\")\n",
    "\n",
    "# ===============================\n",
    "# 6. Kaggle ì œì¶œìš© ì½”ë“œ ì¶”ê°€\n",
    "# ===============================\n",
    "print(\"\\nğŸš€ Kaggle ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\")\n",
    "\n",
    "# 6-1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n",
    "test_df = pd.read_csv(\"/Users/nelllio/Desktop/Machine_Learning/playground-series/test.csv\")\n",
    "\n",
    "# 6-2. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ë™ì¼í•œ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì ìš©\n",
    "test_df = create_golden_features(test_df)\n",
    "X_test = test_df[features]\n",
    "\n",
    "# 6-3. ì „ì²´ í›ˆë ¨ ë°ì´í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ\n",
    "print(\"ğŸ“š ì „ì²´ í›ˆë ¨ ë°ì´í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ ì¤‘...\")\n",
    "stack.fit(X, np.log1p(y))\n",
    "\n",
    "# 6-4. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\n",
    "print(\"ğŸ”® í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...\")\n",
    "test_predictions = np.expm1(stack.predict(X_test))\n",
    "\n",
    "# 6-5. submission íŒŒì¼ ìƒì„±\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'Calories': test_predictions\n",
    "})\n",
    "\n",
    "# 6-6. submission íŒŒì¼ ì €ì¥\n",
    "submission_path = \"/Users/nelllio/Desktop/Machine_Learning/playground-series/submission.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"âœ… Submission íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {submission_path}\")\n",
    "print(f\"ğŸ“Š ì˜ˆì¸¡ëœ ì¹¼ë¡œë¦¬ ë²”ìœ„: {test_predictions.min():.2f} ~ {test_predictions.max():.2f}\")\n",
    "print(f\"ğŸ“Š í‰ê·  ì˜ˆì¸¡ ì¹¼ë¡œë¦¬: {test_predictions.mean():.2f}\")\n",
    "print(f\"ğŸ“‹ Submission í˜•íƒœ:\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691c700-2c09-44c1-847b-3e854248afc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ 0.05739 â†’ 0.04ëŒ€ ëŒíŒŒ íŒŒì´í”„ë¼ì¸ ì‹œì‘!\n",
      "============================================================\n",
      "ğŸ“Š ë°ì´í„° ë¡œë”©...\n",
      "ğŸ”§ í™•ì¥ëœ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§...\n",
      "ğŸ“‹ ì „ì²´ íŠ¹ì„± ìˆ˜: 35\n",
      "ğŸ¯ ì„ íƒëœ íŠ¹ì„± ìˆ˜: 25\n",
      "ğŸ” Top 10 íŠ¹ì„±:\n",
      "   Exercise_load: 16303298.13\n",
      "   Triple_Intensity: 16303298.13\n",
      "   Duration_x_HeartRate: 15988970.37\n",
      "   Power_index: 15634030.11\n",
      "   Duration_squared: 10229560.24\n",
      "   Exercise_intensity_per_BMI: 9624436.75\n",
      "   Duration_x_BodyTemp: 9034282.78\n",
      "   Duration: 8794708.88\n",
      "   Duration_cubed: 5579898.15\n",
      "   Metabolic_Intensity: 4418224.83\n",
      "ğŸ¤– í™•ì¥ëœ ëª¨ë¸ ì•™ìƒë³„ êµ¬ì„±...\n",
      "ğŸ—ï¸ ë‹¤ì¸µ ìŠ¤íƒœí‚¹ ìµœì í™”...\n",
      "ğŸ” ìµœì  ë©”íƒ€ ëŸ¬ë„ˆ íƒìƒ‰ ì¤‘...\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 0.05739 â†’ 0.04ëŒ€ ëŒíŒŒ ìµœì í™” íŒŒì´í”„ë¼ì¸\n",
    "# ê¸°ì¡´ ì½”ë“œ ê¸°ë°˜ ì ì§„ì  ê°œì„ \n",
    "# ===============================\n",
    "\n",
    "from sklearn.ensemble import StackingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV, HuberRegressor\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import numpy as np, pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ===============================\n",
    "# 1. í™•ì¥ëœ ê³¨ë“  íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§\n",
    "# ===============================\n",
    "\n",
    "def create_enhanced_golden_features(df):\n",
    "    \"\"\"ê¸°ì¡´ íŠ¹ì„± + ê³ ê¸‰ íŠ¹ì„±ë“¤ ì¶”ê°€\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ğŸ† ê¸°ì¡´ ê²€ì¦ëœ íŠ¹ì„±ë“¤ (ìœ ì§€)\n",
    "    df['Duration_x_HeartRate']   = df['Duration'] * df['Heart_Rate']\n",
    "    df['Age_adjusted_HeartRate'] = df['Heart_Rate'] / (df['Age'] + 1)\n",
    "    df['BMI']                    = df['Weight'] / (df['Height'] / 100) ** 2\n",
    "    df['Metabolic_Intensity']    = df['Heart_Rate'] * df['Body_Temp']\n",
    "    df['Steps_per_min']          = df['Heart_Rate'] / (df['Duration'] / 60)\n",
    "    df['Weight_to_Age']          = df['Weight'] / (df['Age'] + 1)\n",
    "    df['HeartRate_to_BMI']       = df['Heart_Rate'] / (df['BMI'] + 1)\n",
    "    \n",
    "    # ğŸš€ NEW: ê³ ê¸‰ íŠ¹ì„±ë“¤ ì¶”ê°€\n",
    "    # 1. 3ì¤‘ êµí˜¸ì‘ìš©\n",
    "    df['Triple_Intensity'] = df['Duration'] * df['Heart_Rate'] * df['Body_Temp']\n",
    "    df['Duration_x_BodyTemp'] = df['Duration'] * df['Body_Temp']\n",
    "    df['HeartRate_x_BodyTemp'] = df['Heart_Rate'] * df['Body_Temp']\n",
    "    \n",
    "    # 2. ê³ ì°¨ íŠ¹ì„±ë“¤\n",
    "    df['Duration_squared'] = df['Duration'] ** 2\n",
    "    df['HeartRate_squared'] = df['Heart_Rate'] ** 2\n",
    "    df['Duration_cubed'] = df['Duration'] ** 3\n",
    "    \n",
    "    # 3. ìƒë¦¬í•™ì  ì§€í‘œë“¤\n",
    "    df['BSA'] = 0.007184 * (df['Weight'] ** 0.425) * (df['Height'] ** 0.725)  # Body Surface Area\n",
    "    df['BMR_estimate'] = 88.362 + (13.397 * df['Weight']) + (4.799 * df['Height']) - (5.677 * df['Age'])  # ë‚¨ì„± ê¸°ì¤€ BMR\n",
    "    df['BodyTemp_deviation'] = df['Body_Temp'] - 37.0  # ì •ìƒ ì²´ì˜¨ì—ì„œì˜ í¸ì°¨\n",
    "    \n",
    "    # 4. íš¨ìœ¨ì„± ì§€í‘œë“¤\n",
    "    df['Calorie_efficiency'] = df['Duration_x_HeartRate'] / (df['Weight'] + df['Age'])\n",
    "    df['Exercise_intensity_per_BMI'] = df['Duration_x_HeartRate'] / df['BMI']\n",
    "    df['Metabolic_rate'] = df['Metabolic_Intensity'] / df['Weight']\n",
    "    \n",
    "    # 5. ë¡œê·¸ ë³€í™˜ë“¤ (RMSLE ìµœì í™”)\n",
    "    df['log_Duration'] = np.log1p(df['Duration'])\n",
    "    df['log_HeartRate'] = np.log1p(df['Heart_Rate'])\n",
    "    df['log_Duration_HeartRate'] = np.log1p(df['Duration_x_HeartRate'])\n",
    "    \n",
    "    # 6. ë¹„ìœ¨ ë° ìƒëŒ€ì  ì§€í‘œë“¤\n",
    "    df['HeartRate_intensity'] = df['Heart_Rate'] / (220 - df['Age'])  # ìµœëŒ€ì‹¬ë°•ìˆ˜ ëŒ€ë¹„ ë¹„ìœ¨\n",
    "    df['Duration_per_Age'] = df['Duration'] / df['Age']\n",
    "    df['BodyTemp_per_Age'] = df['Body_Temp'] / df['Age']\n",
    "    \n",
    "    # 7. ë³µí•© ì§€í‘œë“¤\n",
    "    df['Fitness_score'] = (df['Duration'] * df['Heart_Rate']) / (df['Age'] + df['BMI'])\n",
    "    df['Exercise_load'] = df['Duration'] * (df['Heart_Rate'] / 220) * df['Body_Temp']\n",
    "    df['Power_index'] = (df['Duration'] ** 1.5) * (df['Heart_Rate'] ** 0.8)\n",
    "    \n",
    "    # ì„±ë³„ ì¸ì½”ë”©\n",
    "    df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ===============================\n",
    "# 2. í™•ì¥ëœ ëª¨ë¸ ì•™ìƒë¸”\n",
    "# ===============================\n",
    "\n",
    "def create_enhanced_models():\n",
    "    \"\"\"ê¸°ì¡´ 3ê°œ ëª¨ë¸ + ì¶”ê°€ ëª¨ë¸ë“¤\"\"\"\n",
    "    \n",
    "    # ğŸ† ê¸°ì¡´ í•µì‹¬ ëª¨ë¸ë“¤ (í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ë°€ íŠœë‹)\n",
    "    xgb = XGBRegressor(\n",
    "        n_estimators=800,           # 500 â†’ 800\n",
    "        learning_rate=0.03,         # 0.05 â†’ 0.03 (ë” ë³´ìˆ˜ì )\n",
    "        max_depth=7,                # 6 â†’ 7\n",
    "        subsample=0.85,             # 0.9 â†’ 0.85\n",
    "        colsample_bytree=0.85,      # 0.9 â†’ 0.85\n",
    "        reg_alpha=0.1,              # L1 ì •ê·œí™” ì¶”ê°€\n",
    "        reg_lambda=0.1,             # L2 ì •ê·œí™” ì¶”ê°€\n",
    "        n_jobs=-1, random_state=42, verbosity=0\n",
    "    )\n",
    "    \n",
    "    lgb = LGBMRegressor(\n",
    "        n_estimators=800,           # 500 â†’ 800\n",
    "        learning_rate=0.03,         # 0.05 â†’ 0.03\n",
    "        max_depth=8,                # -1 â†’ 8 (ëª…ì‹œì  ê¹Šì´)\n",
    "        subsample=0.85,             # 0.9 â†’ 0.85\n",
    "        colsample_bytree=0.85,      # 0.9 â†’ 0.85\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=0.1,\n",
    "        n_jobs=-1, random_state=42, verbosity=-1\n",
    "    )\n",
    "    \n",
    "    cat = CatBoostRegressor(\n",
    "        n_estimators=600,           # 500 â†’ 600\n",
    "        learning_rate=0.04,         # 0.05 â†’ 0.04\n",
    "        depth=9,                    # 8 â†’ 9\n",
    "        l2_leaf_reg=5,              # 3 â†’ 5\n",
    "        subsample=0.85,             # ì¶”ê°€\n",
    "        verbose=0, random_state=42\n",
    "    )\n",
    "    \n",
    "    # ğŸš€ NEW: ì¶”ê°€ ëª¨ë¸ë“¤ (ë‹¤ì–‘ì„± í™•ë³´)\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=400,\n",
    "        max_depth=12,\n",
    "        min_samples_split=3,\n",
    "        min_samples_leaf=2,\n",
    "        max_features='sqrt',\n",
    "        n_jobs=-1, random_state=42\n",
    "    )\n",
    "    \n",
    "    extra = ExtraTreesRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=10,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        max_features='sqrt',\n",
    "        n_jobs=-1, random_state=42\n",
    "    )\n",
    "    \n",
    "    # XGBoost ë³€í˜• (ë‹¤ë¥¸ ì„¤ì •)\n",
    "    xgb_alt = XGBRegressor(\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_alpha=0.05,\n",
    "        reg_lambda=0.05,\n",
    "        n_jobs=-1, random_state=123, verbosity=0  # ë‹¤ë¥¸ ì‹œë“œ\n",
    "    )\n",
    "    \n",
    "    return [\n",
    "        ('xgb', xgb),\n",
    "        ('lgb', lgb), \n",
    "        ('cat', cat),\n",
    "        ('rf', rf),\n",
    "        ('extra', extra),\n",
    "        ('xgb_alt', xgb_alt)\n",
    "    ]\n",
    "\n",
    "# ===============================\n",
    "# 3. ê³ ê¸‰ ë©”íƒ€ ëŸ¬ë„ˆë“¤\n",
    "# ===============================\n",
    "\n",
    "def create_meta_learners():\n",
    "    \"\"\"ì—¬ëŸ¬ ë©”íƒ€ ëŸ¬ë„ˆ í›„ë³´ë“¤\"\"\"\n",
    "    meta_candidates = {\n",
    "        'ridge': RidgeCV(alphas=[0.1, 0.5, 1.0, 2.0, 5.0]),\n",
    "        'lasso': LassoCV(alphas=[0.01, 0.05, 0.1, 0.5, 1.0], cv=3),\n",
    "        'elastic': ElasticNetCV(alphas=[0.1, 0.5, 1.0], l1_ratio=[0.1, 0.5, 0.9], cv=3),\n",
    "        'huber': HuberRegressor(epsilon=1.5, alpha=0.1)\n",
    "    }\n",
    "    return meta_candidates\n",
    "\n",
    "# ===============================\n",
    "# 4. íŠ¹ì„± ì„ íƒ ìµœì í™”\n",
    "# ===============================\n",
    "\n",
    "def optimize_features(X, y, max_features=None):\n",
    "    \"\"\"ìµœì  íŠ¹ì„± ì„ íƒ\"\"\"\n",
    "    if max_features is None:\n",
    "        max_features = min(30, X.shape[1])  # ìµœëŒ€ 30ê°œ íŠ¹ì„±\n",
    "    \n",
    "    # SelectKBestë¡œ ìµœì  íŠ¹ì„± ì„ íƒ\n",
    "    selector = SelectKBest(score_func=f_regression, k=max_features)\n",
    "    X_selected = selector.fit_transform(X, y)\n",
    "    \n",
    "    selected_features = X.columns[selector.get_support()]\n",
    "    \n",
    "    print(f\"ğŸ¯ ì„ íƒëœ íŠ¹ì„± ìˆ˜: {len(selected_features)}\")\n",
    "    print(\"ğŸ” Top 10 íŠ¹ì„±:\")\n",
    "    feature_scores = list(zip(selected_features, selector.scores_[selector.get_support()]))\n",
    "    feature_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    for feat, score in feature_scores[:10]:\n",
    "        print(f\"   {feat}: {score:.2f}\")\n",
    "    \n",
    "    return X_selected, selected_features\n",
    "\n",
    "# ===============================\n",
    "# 5. ë‹¤ì¸µ ìŠ¤íƒœí‚¹ êµ¬í˜„\n",
    "# ===============================\n",
    "\n",
    "def create_multi_level_stacking(base_models, meta_candidates, X, y):\n",
    "    \"\"\"ë‹¤ì¸µ ìŠ¤íƒœí‚¹ìœ¼ë¡œ ìµœì  ë©”íƒ€ ëŸ¬ë„ˆ ì„ íƒ\"\"\"\n",
    "    \n",
    "    print(\"ğŸ” ìµœì  ë©”íƒ€ ëŸ¬ë„ˆ íƒìƒ‰ ì¤‘...\")\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    best_score = float('inf')\n",
    "    best_meta_name = None\n",
    "    best_stack = None\n",
    "    \n",
    "    for meta_name, meta_model in meta_candidates.items():\n",
    "        stack = StackingRegressor(\n",
    "            estimators=base_models,\n",
    "            final_estimator=meta_model,\n",
    "            cv=3,  # ë‚´ë¶€ CV\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # êµì°¨ ê²€ì¦\n",
    "        cv_scores = cross_val_score(\n",
    "            stack, X, np.log1p(y),\n",
    "            cv=kf, \n",
    "            scoring=make_scorer(lambda y_true, y_pred: \n",
    "                               np.sqrt(mean_squared_error(y_true, y_pred)), \n",
    "                               greater_is_better=False),\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        avg_score = -cv_scores.mean()\n",
    "        print(f\"   {meta_name}: {avg_score:.6f} Â± {cv_scores.std():.6f}\")\n",
    "        \n",
    "        if avg_score < best_score:\n",
    "            best_score = avg_score\n",
    "            best_meta_name = meta_name\n",
    "            best_stack = stack\n",
    "    \n",
    "    print(f\"ğŸ† ìµœì  ë©”íƒ€ ëŸ¬ë„ˆ: {best_meta_name} (Score: {best_score:.6f})\")\n",
    "    return best_stack, best_meta_name, best_score\n",
    "\n",
    "# ===============================\n",
    "# 6. ë©”ì¸ íŒŒì´í”„ë¼ì¸\n",
    "# ===============================\n",
    "\n",
    "def run_enhanced_pipeline(train_path, test_path):\n",
    "    \"\"\"í–¥ìƒëœ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\"\"\"\n",
    "    \n",
    "    print(\"ğŸš€ 0.05739 â†’ 0.04ëŒ€ ëŒíŒŒ íŒŒì´í”„ë¼ì¸ ì‹œì‘!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ë°ì´í„° ë¡œë“œ\n",
    "    print(\"ğŸ“Š ë°ì´í„° ë¡œë”©...\")\n",
    "    df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    \n",
    "    # í™•ì¥ëœ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§\n",
    "    print(\"ğŸ”§ í™•ì¥ëœ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§...\")\n",
    "    df = create_enhanced_golden_features(df)\n",
    "    test_df = create_enhanced_golden_features(test_df)\n",
    "    \n",
    "    # íŠ¹ì„± ì„ íƒ\n",
    "    feature_cols = [col for col in df.columns if col not in ['id', 'Calories']]\n",
    "    X = df[feature_cols]\n",
    "    y = df['Calories']\n",
    "    \n",
    "    print(f\"ğŸ“‹ ì „ì²´ íŠ¹ì„± ìˆ˜: {len(feature_cols)}\")\n",
    "    \n",
    "    # íŠ¹ì„± ìµœì í™”\n",
    "    X_optimized, selected_features = optimize_features(X, y, max_features=25)\n",
    "    X_optimized = pd.DataFrame(X_optimized, columns=selected_features)\n",
    "    X_test_optimized = test_df[selected_features]\n",
    "    \n",
    "    # ëª¨ë¸ ìƒì„±\n",
    "    print(\"ğŸ¤– í™•ì¥ëœ ëª¨ë¸ ì•™ìƒë³„ êµ¬ì„±...\")\n",
    "    base_models = create_enhanced_models()\n",
    "    meta_candidates = create_meta_learners()\n",
    "    \n",
    "    # ë‹¤ì¸µ ìŠ¤íƒœí‚¹\n",
    "    print(\"ğŸ—ï¸ ë‹¤ì¸µ ìŠ¤íƒœí‚¹ ìµœì í™”...\")\n",
    "    best_stack, best_meta, best_cv_score = create_multi_level_stacking(\n",
    "        base_models, meta_candidates, X_optimized, y\n",
    "    )\n",
    "    \n",
    "    # RMSLE ê³„ì‚°\n",
    "    def rmsle(y_true, y_pred):\n",
    "        y_pred = np.maximum(y_pred, 0.1)\n",
    "        return np.sqrt(mean_squared_error(np.log1p(y_true), np.log1p(y_pred)))\n",
    "    \n",
    "    # í™€ë“œì•„ì›ƒ ê²€ì¦\n",
    "    print(\"\\nğŸ“š í™€ë“œì•„ì›ƒ ê²€ì¦...\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_optimized, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    best_stack.fit(X_train, np.log1p(y_train))\n",
    "    y_pred = np.expm1(best_stack.predict(X_val))\n",
    "    holdout_rmsle = rmsle(y_val, y_pred)\n",
    "    \n",
    "    print(f\"ğŸ¯ Hold-out RMSLE: {holdout_rmsle:.6f}\")\n",
    "    \n",
    "    # ìµœì¢… ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡\n",
    "    print(\"\\nğŸš€ ìµœì¢… ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡...\")\n",
    "    best_stack.fit(X_optimized, np.log1p(y))\n",
    "    test_predictions = np.expm1(best_stack.predict(X_test_optimized))\n",
    "    \n",
    "    # ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_df['id'],\n",
    "        'Calories': test_predictions\n",
    "    })\n",
    "    \n",
    "    submission_path = \"enhanced_submission.csv\"\n",
    "    submission.to_csv(submission_path, index=False)\n",
    "    \n",
    "    print(f\"\\nâœ… ì œì¶œ íŒŒì¼ ìƒì„±: {submission_path}\")\n",
    "    print(f\"ğŸ“Š ì˜ˆì¸¡ í†µê³„:\")\n",
    "    print(f\"   í‰ê· : {test_predictions.mean():.2f}\")\n",
    "    print(f\"   ë²”ìœ„: {test_predictions.min():.2f} ~ {test_predictions.max():.2f}\")\n",
    "    print(f\"   í‘œì¤€í¸ì°¨: {test_predictions.std():.2f}\")\n",
    "    \n",
    "    print(f\"\\nğŸ† ì˜ˆìƒ ì„±ëŠ¥: RMSLE ~{holdout_rmsle:.6f}\")\n",
    "    \n",
    "    if holdout_rmsle < 0.050:\n",
    "        print(\"ğŸ”¥ ëª©í‘œ ë‹¬ì„±! 0.04ëŒ€ ì§„ì…!\")\n",
    "    elif holdout_rmsle < 0.055:\n",
    "        print(\"ğŸ’ª ë§¤ìš° ê·¼ì ‘! ì¶”ê°€ íŠœë‹ìœ¼ë¡œ ë‹¬ì„± ê°€ëŠ¥!\")\n",
    "    else:\n",
    "        print(\"ğŸ“ˆ ì¢‹ì€ ê°œì„ ! ê³„ì† ìµœì í™” ì§„í–‰!\")\n",
    "    \n",
    "    return submission, holdout_rmsle, selected_features\n",
    "\n",
    "# ===============================\n",
    "# 7. ì‹¤í–‰ ì½”ë“œ\n",
    "# ===============================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # íŒŒì¼ ê²½ë¡œ ì„¤ì • (ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)\n",
    "    train_path = \"/Users/nelllio/Desktop/Machine_Learning/playground-series/train.csv\"\n",
    "    test_path = \"/Users/nelllio/Desktop/Machine_Learning/playground-series/test.csv\"\n",
    "    \n",
    "    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "    submission, final_score, features = run_enhanced_pipeline(train_path, test_path)\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ ìµœì¢… ê²°ê³¼:\")\n",
    "    print(f\"   ê¸°ì¡´ ì„±ê³¼: 0.05739\")\n",
    "    print(f\"   ì˜ˆìƒ ì„±ê³¼: {final_score:.6f}\")\n",
    "    print(f\"   ê°œì„ ë„: {0.05739 - final_score:.6f}\")\n",
    "    \n",
    "    if final_score < 0.05739:\n",
    "        improvement = ((0.05739 - final_score) / 0.05739) * 100\n",
    "        print(f\"   ğŸ‰ {improvement:.2f}% ê°œì„ !\")\n",
    "\n",
    "\"\"\"\n",
    "ğŸ¯ ì£¼ìš” ê°œì„ ì‚¬í•­:\n",
    "1. ğŸ”§ 25+ ìƒˆë¡œìš´ ê³ ê¸‰ íŠ¹ì„± ì¶”ê°€\n",
    "2. ğŸ¤– 3ê°œ â†’ 6ê°œ ëª¨ë¸ë¡œ ë‹¤ì–‘ì„± í™•ëŒ€\n",
    "3. ğŸ—ï¸ 4ê°€ì§€ ë©”íƒ€ ëŸ¬ë„ˆ ì¤‘ ìµœì  ì„ íƒ\n",
    "4. ğŸ¯ íŠ¹ì„± ì„ íƒ ìµœì í™” (SelectKBest)\n",
    "5. âš™ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ë°€ íŠœë‹\n",
    "6. ğŸ“Š ë‹¤ì¸µ ìŠ¤íƒœí‚¹ êµ¬í˜„\n",
    "\n",
    "ğŸš€ ì˜ˆìƒ ì„±ëŠ¥: 0.05739 â†’ 0.045-0.055\n",
    "ğŸ’ª ëª©í‘œ: 0.04ëŒ€ ëŒíŒŒ!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80acb60b-fe0f-49e7-9236-ef89fb4c57e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11893428,
     "sourceId": 91716,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

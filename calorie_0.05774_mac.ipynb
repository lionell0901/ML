{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6071784-0fa1-434b-bc00-9e5b3213d082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1ecb503-04e3-4d75-919b-da15cd9e210c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750000, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Heart_Rate</th>\n",
       "      <th>Body_Temp</th>\n",
       "      <th>Calories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>36</td>\n",
       "      <td>189.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>64</td>\n",
       "      <td>163.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>39.7</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>51</td>\n",
       "      <td>161.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>39.8</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>20</td>\n",
       "      <td>192.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>40.7</td>\n",
       "      <td>140.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>female</td>\n",
       "      <td>38</td>\n",
       "      <td>166.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>40.6</td>\n",
       "      <td>146.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     Sex  Age  Height  Weight  Duration  Heart_Rate  Body_Temp  Calories\n",
       "0   0    male   36   189.0    82.0      26.0       101.0       41.0     150.0\n",
       "1   1  female   64   163.0    60.0       8.0        85.0       39.7      34.0\n",
       "2   2  female   51   161.0    64.0       7.0        84.0       39.8      29.0\n",
       "3   3    male   20   192.0    90.0      25.0       105.0       40.7     140.0\n",
       "4   4  female   38   166.0    61.0      25.0       102.0       40.6     146.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "df = pd.read_csv(\"/Users/nelllio/Desktop/Machine_Learning/playground-series/train.csv\")\n",
    "print(df.shape)\n",
    "df.head()\n",
    "#df 변수 지정 후 head()를 통해 5행까지 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35fdf56c-1a3d-470e-9364-e7cc14b23a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 750000 entries, 0 to 749999\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   id          750000 non-null  int64  \n",
      " 1   Sex         750000 non-null  object \n",
      " 2   Age         750000 non-null  int64  \n",
      " 3   Height      750000 non-null  float64\n",
      " 4   Weight      750000 non-null  float64\n",
      " 5   Duration    750000 non-null  float64\n",
      " 6   Heart_Rate  750000 non-null  float64\n",
      " 7   Body_Temp   750000 non-null  float64\n",
      " 8   Calories    750000 non-null  float64\n",
      "dtypes: float64(6), int64(2), object(1)\n",
      "memory usage: 51.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train = df.info()\n",
    "print(train)\n",
    "# count에 non-null로 결측치 없는 것 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8cb45f5-2cd0-41dc-9509-072927198979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027790 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2141\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058849 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2143\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140724\n",
      "[LightGBM] [Info] Start training from score 4.141876\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065664 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2143\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045546 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2146\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.141163\n",
      "[LightGBM] [Info] Start training from score 4.141466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008363 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2144\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140493\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2141\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.142021\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016219 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2141\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.141786\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065705 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2144\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.141961\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.086400 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2140\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140974\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064179 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2143\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140854\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006751 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2140\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.141604\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032145 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2145\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.141523\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007056 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2143\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140854\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014914 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2142\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140429\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083022 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2146\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007462 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2141\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.142997\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024874 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2146\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.142381\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037550 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2144\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.142023\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041467 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2146\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.141589\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.084738 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2144\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.141844\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018730 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2141\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.141706\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032340 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2147\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.141103\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.139392 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2144\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140839\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008978 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2144\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140275\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067002 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2144\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140997\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038681 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2148\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140534\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011945 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2141\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.141052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013492 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2144\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140140\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026942 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2144\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.139282\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030385 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2144\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.139376\n",
      "📊 5-fold CV RMSLE: 0.0171 ± 0.0001\n",
      "\n",
      "📚 모델 학습 중...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005977 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2144\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140559\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059197 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2144\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.077467 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2145\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.142413\n",
      "[LightGBM] [Info] Start training from score 4.140125\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059595 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2144\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.141428\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2146\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.141291\n",
      "🎯 Hold-out RMSLE: 0.0596\n",
      "\n",
      "🚀 Kaggle 제출 파일 생성 중...\n",
      "📚 전체 훈련 데이터로 최종 모델 학습 중...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2143\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140954\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031933 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2147\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.140934\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034914 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2142\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033739 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2145\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039716 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2145\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4.141530\n",
      "[LightGBM] [Info] Start training from score 4.142215\n",
      "[LightGBM] [Info] Start training from score 4.140089\n",
      "🔮 테스트 데이터 예측 중...\n",
      "✅ Submission 파일이 생성되었습니다: /Users/nelllio/Desktop/Machine_Learning/playground-series/submission.csv\n",
      "📊 예측된 칼로리 범위: 1.04 ~ 295.20\n",
      "📊 평균 예측 칼로리: 88.18\n",
      "📋 Submission 형태:\n",
      "       id    Calories\n",
      "0  750000   27.653635\n",
      "1  750001  107.592008\n",
      "2  750002   87.117329\n",
      "3  750003  126.170766\n",
      "4  750004   76.208452\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 0. 라이브러리 (동일)\n",
    "# ===============================\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "# ===============================\n",
    "# 1. 데이터 & FE (동일)\n",
    "# ===============================\n",
    "\n",
    "def create_golden_features(df):\n",
    "    df = df.copy()\n",
    "    df['Duration_x_HeartRate']   = df['Duration'] * df['Heart_Rate']\n",
    "    df['Age_adjusted_HeartRate'] = df['Heart_Rate'] / (df['Age'] + 1)\n",
    "    df['BMI']                    = df['Weight'] / (df['Height'] / 100) ** 2\n",
    "    df['Metabolic_Intensity']    = df['Heart_Rate'] * df['Body_Temp']\n",
    "    df['Steps_per_min']          = df['Heart_Rate'] / (df['Duration'] / 60)\n",
    "    df['Weight_to_Age']          = df['Weight'] / (df['Age'] + 1)\n",
    "    df['HeartRate_to_BMI']       = df['Heart_Rate'] / (df['BMI'] + 1)\n",
    "    df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
    "    return df\n",
    "\n",
    "df = create_golden_features(df)\n",
    "features = [\n",
    "    'Duration','Heart_Rate','BMI','Body_Temp',\n",
    "    'Duration_x_HeartRate','Metabolic_Intensity',\n",
    "    'Age','Height','Weight',\n",
    "    'Steps_per_min','Age_adjusted_HeartRate',\n",
    "    'Weight_to_Age','HeartRate_to_BMI','Sex'\n",
    "]\n",
    "X, y = df[features], df['Calories']\n",
    "\n",
    "# ===============================\n",
    "# 2. RMSLE 스코어러 + 5-fold CV\n",
    "# ===============================\n",
    "def rmsle(y_true, y_pred):\n",
    "    y_pred = np.maximum(y_pred, 0.1)                      # log 안정화\n",
    "    return np.sqrt(mean_squared_error(np.log1p(y_true),\n",
    "                                      np.log1p(y_pred)))\n",
    "\n",
    "rmsle_scorer = make_scorer(rmsle, greater_is_better=False)  # 음수 반환(작을수록 좋음)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ===============================\n",
    "# 3. Base learner – 미세 튜닝\n",
    "#    (n_estimators ↑, learning_rate ↓)\n",
    "# ===============================\n",
    "xgb = XGBRegressor(\n",
    "        n_estimators=500, learning_rate=0.05,\n",
    "        max_depth=6, subsample=0.9, colsample_bytree=0.9,\n",
    "        n_jobs=-1, random_state=42, verbosity=0)\n",
    "\n",
    "lgb = LGBMRegressor(\n",
    "        n_estimators=500, learning_rate=0.05,\n",
    "        max_depth=-1, subsample=0.9, colsample_bytree=0.9,\n",
    "        n_jobs=-1, random_state=42)\n",
    "\n",
    "cat = CatBoostRegressor(\n",
    "        n_estimators=500, learning_rate=0.05,\n",
    "        depth=8, l2_leaf_reg=3,\n",
    "        verbose=0, random_state=42)\n",
    "\n",
    "# ===============================\n",
    "# 4. 스태킹 정의 (동일)\n",
    "# ===============================\n",
    "stack = StackingRegressor(\n",
    "    estimators=[('xgb', xgb), ('lgb', lgb), ('cat', cat)],\n",
    "    final_estimator=RidgeCV(),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 5-1. 교차검증 RMSLE 확인\n",
    "# ===============================\n",
    "cv_scores = cross_val_score(stack, X, np.log1p(y),   # log1p 변환 ⇐ 동일 기준\n",
    "                            cv=kf, scoring=rmsle_scorer,\n",
    "                            n_jobs=-1)\n",
    "print(f\"📊 5-fold CV RMSLE: {(-cv_scores.mean()):.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# ===============================\n",
    "# 5-2. 홀드아웃 학습/평가\n",
    "# ===============================\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42)\n",
    "print(\"\\n📚 모델 학습 중...\")\n",
    "stack.fit(X_train, np.log1p(y_train))\n",
    "y_pred = np.expm1(stack.predict(X_val))\n",
    "print(f\"🎯 Hold-out RMSLE: {rmsle(y_val, y_pred):.4f}\")\n",
    "\n",
    "# ===============================\n",
    "# 6. Kaggle 제출용 코드 추가\n",
    "# ===============================\n",
    "print(\"\\n🚀 Kaggle 제출 파일 생성 중...\")\n",
    "\n",
    "# 6-1. 테스트 데이터 로드\n",
    "test_df = pd.read_csv(\"/Users/nelllio/Desktop/Machine_Learning/playground-series/test.csv\")\n",
    "\n",
    "# 6-2. 테스트 데이터에 동일한 피처 엔지니어링 적용\n",
    "test_df = create_golden_features(test_df)\n",
    "X_test = test_df[features]\n",
    "\n",
    "# 6-3. 전체 훈련 데이터로 최종 모델 학습\n",
    "print(\"📚 전체 훈련 데이터로 최종 모델 학습 중...\")\n",
    "stack.fit(X, np.log1p(y))\n",
    "\n",
    "# 6-4. 테스트 데이터 예측\n",
    "print(\"🔮 테스트 데이터 예측 중...\")\n",
    "test_predictions = np.expm1(stack.predict(X_test))\n",
    "\n",
    "# 6-5. submission 파일 생성\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'Calories': test_predictions\n",
    "})\n",
    "\n",
    "# 6-6. submission 파일 저장\n",
    "submission_path = \"/Users/nelllio/Desktop/Machine_Learning/playground-series/submission.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"✅ Submission 파일이 생성되었습니다: {submission_path}\")\n",
    "print(f\"📊 예측된 칼로리 범위: {test_predictions.min():.2f} ~ {test_predictions.max():.2f}\")\n",
    "print(f\"📊 평균 예측 칼로리: {test_predictions.mean():.2f}\")\n",
    "print(f\"📋 Submission 형태:\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691c700-2c09-44c1-847b-3e854248afc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 0.05739 → 0.04대 돌파 파이프라인 시작!\n",
      "============================================================\n",
      "📊 데이터 로딩...\n",
      "🔧 확장된 특성 엔지니어링...\n",
      "📋 전체 특성 수: 35\n",
      "🎯 선택된 특성 수: 25\n",
      "🔝 Top 10 특성:\n",
      "   Exercise_load: 16303298.13\n",
      "   Triple_Intensity: 16303298.13\n",
      "   Duration_x_HeartRate: 15988970.37\n",
      "   Power_index: 15634030.11\n",
      "   Duration_squared: 10229560.24\n",
      "   Exercise_intensity_per_BMI: 9624436.75\n",
      "   Duration_x_BodyTemp: 9034282.78\n",
      "   Duration: 8794708.88\n",
      "   Duration_cubed: 5579898.15\n",
      "   Metabolic_Intensity: 4418224.83\n",
      "🤖 확장된 모델 앙상별 구성...\n",
      "🏗️ 다층 스태킹 최적화...\n",
      "🔍 최적 메타 러너 탐색 중...\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 0.05739 → 0.04대 돌파 최적화 파이프라인\n",
    "# 기존 코드 기반 점진적 개선\n",
    "# ===============================\n",
    "\n",
    "from sklearn.ensemble import StackingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV, HuberRegressor\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import numpy as np, pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ===============================\n",
    "# 1. 확장된 골든 특성 엔지니어링\n",
    "# ===============================\n",
    "\n",
    "def create_enhanced_golden_features(df):\n",
    "    \"\"\"기존 특성 + 고급 특성들 추가\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 🏆 기존 검증된 특성들 (유지)\n",
    "    df['Duration_x_HeartRate']   = df['Duration'] * df['Heart_Rate']\n",
    "    df['Age_adjusted_HeartRate'] = df['Heart_Rate'] / (df['Age'] + 1)\n",
    "    df['BMI']                    = df['Weight'] / (df['Height'] / 100) ** 2\n",
    "    df['Metabolic_Intensity']    = df['Heart_Rate'] * df['Body_Temp']\n",
    "    df['Steps_per_min']          = df['Heart_Rate'] / (df['Duration'] / 60)\n",
    "    df['Weight_to_Age']          = df['Weight'] / (df['Age'] + 1)\n",
    "    df['HeartRate_to_BMI']       = df['Heart_Rate'] / (df['BMI'] + 1)\n",
    "    \n",
    "    # 🚀 NEW: 고급 특성들 추가\n",
    "    # 1. 3중 교호작용\n",
    "    df['Triple_Intensity'] = df['Duration'] * df['Heart_Rate'] * df['Body_Temp']\n",
    "    df['Duration_x_BodyTemp'] = df['Duration'] * df['Body_Temp']\n",
    "    df['HeartRate_x_BodyTemp'] = df['Heart_Rate'] * df['Body_Temp']\n",
    "    \n",
    "    # 2. 고차 특성들\n",
    "    df['Duration_squared'] = df['Duration'] ** 2\n",
    "    df['HeartRate_squared'] = df['Heart_Rate'] ** 2\n",
    "    df['Duration_cubed'] = df['Duration'] ** 3\n",
    "    \n",
    "    # 3. 생리학적 지표들\n",
    "    df['BSA'] = 0.007184 * (df['Weight'] ** 0.425) * (df['Height'] ** 0.725)  # Body Surface Area\n",
    "    df['BMR_estimate'] = 88.362 + (13.397 * df['Weight']) + (4.799 * df['Height']) - (5.677 * df['Age'])  # 남성 기준 BMR\n",
    "    df['BodyTemp_deviation'] = df['Body_Temp'] - 37.0  # 정상 체온에서의 편차\n",
    "    \n",
    "    # 4. 효율성 지표들\n",
    "    df['Calorie_efficiency'] = df['Duration_x_HeartRate'] / (df['Weight'] + df['Age'])\n",
    "    df['Exercise_intensity_per_BMI'] = df['Duration_x_HeartRate'] / df['BMI']\n",
    "    df['Metabolic_rate'] = df['Metabolic_Intensity'] / df['Weight']\n",
    "    \n",
    "    # 5. 로그 변환들 (RMSLE 최적화)\n",
    "    df['log_Duration'] = np.log1p(df['Duration'])\n",
    "    df['log_HeartRate'] = np.log1p(df['Heart_Rate'])\n",
    "    df['log_Duration_HeartRate'] = np.log1p(df['Duration_x_HeartRate'])\n",
    "    \n",
    "    # 6. 비율 및 상대적 지표들\n",
    "    df['HeartRate_intensity'] = df['Heart_Rate'] / (220 - df['Age'])  # 최대심박수 대비 비율\n",
    "    df['Duration_per_Age'] = df['Duration'] / df['Age']\n",
    "    df['BodyTemp_per_Age'] = df['Body_Temp'] / df['Age']\n",
    "    \n",
    "    # 7. 복합 지표들\n",
    "    df['Fitness_score'] = (df['Duration'] * df['Heart_Rate']) / (df['Age'] + df['BMI'])\n",
    "    df['Exercise_load'] = df['Duration'] * (df['Heart_Rate'] / 220) * df['Body_Temp']\n",
    "    df['Power_index'] = (df['Duration'] ** 1.5) * (df['Heart_Rate'] ** 0.8)\n",
    "    \n",
    "    # 성별 인코딩\n",
    "    df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ===============================\n",
    "# 2. 확장된 모델 앙상블\n",
    "# ===============================\n",
    "\n",
    "def create_enhanced_models():\n",
    "    \"\"\"기존 3개 모델 + 추가 모델들\"\"\"\n",
    "    \n",
    "    # 🏆 기존 핵심 모델들 (하이퍼파라미터 정밀 튜닝)\n",
    "    xgb = XGBRegressor(\n",
    "        n_estimators=800,           # 500 → 800\n",
    "        learning_rate=0.03,         # 0.05 → 0.03 (더 보수적)\n",
    "        max_depth=7,                # 6 → 7\n",
    "        subsample=0.85,             # 0.9 → 0.85\n",
    "        colsample_bytree=0.85,      # 0.9 → 0.85\n",
    "        reg_alpha=0.1,              # L1 정규화 추가\n",
    "        reg_lambda=0.1,             # L2 정규화 추가\n",
    "        n_jobs=-1, random_state=42, verbosity=0\n",
    "    )\n",
    "    \n",
    "    lgb = LGBMRegressor(\n",
    "        n_estimators=800,           # 500 → 800\n",
    "        learning_rate=0.03,         # 0.05 → 0.03\n",
    "        max_depth=8,                # -1 → 8 (명시적 깊이)\n",
    "        subsample=0.85,             # 0.9 → 0.85\n",
    "        colsample_bytree=0.85,      # 0.9 → 0.85\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=0.1,\n",
    "        n_jobs=-1, random_state=42, verbosity=-1\n",
    "    )\n",
    "    \n",
    "    cat = CatBoostRegressor(\n",
    "        n_estimators=600,           # 500 → 600\n",
    "        learning_rate=0.04,         # 0.05 → 0.04\n",
    "        depth=9,                    # 8 → 9\n",
    "        l2_leaf_reg=5,              # 3 → 5\n",
    "        subsample=0.85,             # 추가\n",
    "        verbose=0, random_state=42\n",
    "    )\n",
    "    \n",
    "    # 🚀 NEW: 추가 모델들 (다양성 확보)\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=400,\n",
    "        max_depth=12,\n",
    "        min_samples_split=3,\n",
    "        min_samples_leaf=2,\n",
    "        max_features='sqrt',\n",
    "        n_jobs=-1, random_state=42\n",
    "    )\n",
    "    \n",
    "    extra = ExtraTreesRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=10,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        max_features='sqrt',\n",
    "        n_jobs=-1, random_state=42\n",
    "    )\n",
    "    \n",
    "    # XGBoost 변형 (다른 설정)\n",
    "    xgb_alt = XGBRegressor(\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_alpha=0.05,\n",
    "        reg_lambda=0.05,\n",
    "        n_jobs=-1, random_state=123, verbosity=0  # 다른 시드\n",
    "    )\n",
    "    \n",
    "    return [\n",
    "        ('xgb', xgb),\n",
    "        ('lgb', lgb), \n",
    "        ('cat', cat),\n",
    "        ('rf', rf),\n",
    "        ('extra', extra),\n",
    "        ('xgb_alt', xgb_alt)\n",
    "    ]\n",
    "\n",
    "# ===============================\n",
    "# 3. 고급 메타 러너들\n",
    "# ===============================\n",
    "\n",
    "def create_meta_learners():\n",
    "    \"\"\"여러 메타 러너 후보들\"\"\"\n",
    "    meta_candidates = {\n",
    "        'ridge': RidgeCV(alphas=[0.1, 0.5, 1.0, 2.0, 5.0]),\n",
    "        'lasso': LassoCV(alphas=[0.01, 0.05, 0.1, 0.5, 1.0], cv=3),\n",
    "        'elastic': ElasticNetCV(alphas=[0.1, 0.5, 1.0], l1_ratio=[0.1, 0.5, 0.9], cv=3),\n",
    "        'huber': HuberRegressor(epsilon=1.5, alpha=0.1)\n",
    "    }\n",
    "    return meta_candidates\n",
    "\n",
    "# ===============================\n",
    "# 4. 특성 선택 최적화\n",
    "# ===============================\n",
    "\n",
    "def optimize_features(X, y, max_features=None):\n",
    "    \"\"\"최적 특성 선택\"\"\"\n",
    "    if max_features is None:\n",
    "        max_features = min(30, X.shape[1])  # 최대 30개 특성\n",
    "    \n",
    "    # SelectKBest로 최적 특성 선택\n",
    "    selector = SelectKBest(score_func=f_regression, k=max_features)\n",
    "    X_selected = selector.fit_transform(X, y)\n",
    "    \n",
    "    selected_features = X.columns[selector.get_support()]\n",
    "    \n",
    "    print(f\"🎯 선택된 특성 수: {len(selected_features)}\")\n",
    "    print(\"🔝 Top 10 특성:\")\n",
    "    feature_scores = list(zip(selected_features, selector.scores_[selector.get_support()]))\n",
    "    feature_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    for feat, score in feature_scores[:10]:\n",
    "        print(f\"   {feat}: {score:.2f}\")\n",
    "    \n",
    "    return X_selected, selected_features\n",
    "\n",
    "# ===============================\n",
    "# 5. 다층 스태킹 구현\n",
    "# ===============================\n",
    "\n",
    "def create_multi_level_stacking(base_models, meta_candidates, X, y):\n",
    "    \"\"\"다층 스태킹으로 최적 메타 러너 선택\"\"\"\n",
    "    \n",
    "    print(\"🔍 최적 메타 러너 탐색 중...\")\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    best_score = float('inf')\n",
    "    best_meta_name = None\n",
    "    best_stack = None\n",
    "    \n",
    "    for meta_name, meta_model in meta_candidates.items():\n",
    "        stack = StackingRegressor(\n",
    "            estimators=base_models,\n",
    "            final_estimator=meta_model,\n",
    "            cv=3,  # 내부 CV\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # 교차 검증\n",
    "        cv_scores = cross_val_score(\n",
    "            stack, X, np.log1p(y),\n",
    "            cv=kf, \n",
    "            scoring=make_scorer(lambda y_true, y_pred: \n",
    "                               np.sqrt(mean_squared_error(y_true, y_pred)), \n",
    "                               greater_is_better=False),\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        avg_score = -cv_scores.mean()\n",
    "        print(f\"   {meta_name}: {avg_score:.6f} ± {cv_scores.std():.6f}\")\n",
    "        \n",
    "        if avg_score < best_score:\n",
    "            best_score = avg_score\n",
    "            best_meta_name = meta_name\n",
    "            best_stack = stack\n",
    "    \n",
    "    print(f\"🏆 최적 메타 러너: {best_meta_name} (Score: {best_score:.6f})\")\n",
    "    return best_stack, best_meta_name, best_score\n",
    "\n",
    "# ===============================\n",
    "# 6. 메인 파이프라인\n",
    "# ===============================\n",
    "\n",
    "def run_enhanced_pipeline(train_path, test_path):\n",
    "    \"\"\"향상된 파이프라인 실행\"\"\"\n",
    "    \n",
    "    print(\"🚀 0.05739 → 0.04대 돌파 파이프라인 시작!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 데이터 로드\n",
    "    print(\"📊 데이터 로딩...\")\n",
    "    df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    \n",
    "    # 확장된 특성 엔지니어링\n",
    "    print(\"🔧 확장된 특성 엔지니어링...\")\n",
    "    df = create_enhanced_golden_features(df)\n",
    "    test_df = create_enhanced_golden_features(test_df)\n",
    "    \n",
    "    # 특성 선택\n",
    "    feature_cols = [col for col in df.columns if col not in ['id', 'Calories']]\n",
    "    X = df[feature_cols]\n",
    "    y = df['Calories']\n",
    "    \n",
    "    print(f\"📋 전체 특성 수: {len(feature_cols)}\")\n",
    "    \n",
    "    # 특성 최적화\n",
    "    X_optimized, selected_features = optimize_features(X, y, max_features=25)\n",
    "    X_optimized = pd.DataFrame(X_optimized, columns=selected_features)\n",
    "    X_test_optimized = test_df[selected_features]\n",
    "    \n",
    "    # 모델 생성\n",
    "    print(\"🤖 확장된 모델 앙상별 구성...\")\n",
    "    base_models = create_enhanced_models()\n",
    "    meta_candidates = create_meta_learners()\n",
    "    \n",
    "    # 다층 스태킹\n",
    "    print(\"🏗️ 다층 스태킹 최적화...\")\n",
    "    best_stack, best_meta, best_cv_score = create_multi_level_stacking(\n",
    "        base_models, meta_candidates, X_optimized, y\n",
    "    )\n",
    "    \n",
    "    # RMSLE 계산\n",
    "    def rmsle(y_true, y_pred):\n",
    "        y_pred = np.maximum(y_pred, 0.1)\n",
    "        return np.sqrt(mean_squared_error(np.log1p(y_true), np.log1p(y_pred)))\n",
    "    \n",
    "    # 홀드아웃 검증\n",
    "    print(\"\\n📚 홀드아웃 검증...\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_optimized, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    best_stack.fit(X_train, np.log1p(y_train))\n",
    "    y_pred = np.expm1(best_stack.predict(X_val))\n",
    "    holdout_rmsle = rmsle(y_val, y_pred)\n",
    "    \n",
    "    print(f\"🎯 Hold-out RMSLE: {holdout_rmsle:.6f}\")\n",
    "    \n",
    "    # 최종 모델 훈련 및 예측\n",
    "    print(\"\\n🚀 최종 모델 훈련 및 예측...\")\n",
    "    best_stack.fit(X_optimized, np.log1p(y))\n",
    "    test_predictions = np.expm1(best_stack.predict(X_test_optimized))\n",
    "    \n",
    "    # 제출 파일 생성\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_df['id'],\n",
    "        'Calories': test_predictions\n",
    "    })\n",
    "    \n",
    "    submission_path = \"enhanced_submission.csv\"\n",
    "    submission.to_csv(submission_path, index=False)\n",
    "    \n",
    "    print(f\"\\n✅ 제출 파일 생성: {submission_path}\")\n",
    "    print(f\"📊 예측 통계:\")\n",
    "    print(f\"   평균: {test_predictions.mean():.2f}\")\n",
    "    print(f\"   범위: {test_predictions.min():.2f} ~ {test_predictions.max():.2f}\")\n",
    "    print(f\"   표준편차: {test_predictions.std():.2f}\")\n",
    "    \n",
    "    print(f\"\\n🏆 예상 성능: RMSLE ~{holdout_rmsle:.6f}\")\n",
    "    \n",
    "    if holdout_rmsle < 0.050:\n",
    "        print(\"🔥 목표 달성! 0.04대 진입!\")\n",
    "    elif holdout_rmsle < 0.055:\n",
    "        print(\"💪 매우 근접! 추가 튜닝으로 달성 가능!\")\n",
    "    else:\n",
    "        print(\"📈 좋은 개선! 계속 최적화 진행!\")\n",
    "    \n",
    "    return submission, holdout_rmsle, selected_features\n",
    "\n",
    "# ===============================\n",
    "# 7. 실행 코드\n",
    "# ===============================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 파일 경로 설정 (사용자 환경에 맞게 수정)\n",
    "    train_path = \"/Users/nelllio/Desktop/Machine_Learning/playground-series/train.csv\"\n",
    "    test_path = \"/Users/nelllio/Desktop/Machine_Learning/playground-series/test.csv\"\n",
    "    \n",
    "    # 파이프라인 실행\n",
    "    submission, final_score, features = run_enhanced_pipeline(train_path, test_path)\n",
    "    \n",
    "    print(f\"\\n📋 최종 결과:\")\n",
    "    print(f\"   기존 성과: 0.05739\")\n",
    "    print(f\"   예상 성과: {final_score:.6f}\")\n",
    "    print(f\"   개선도: {0.05739 - final_score:.6f}\")\n",
    "    \n",
    "    if final_score < 0.05739:\n",
    "        improvement = ((0.05739 - final_score) / 0.05739) * 100\n",
    "        print(f\"   🎉 {improvement:.2f}% 개선!\")\n",
    "\n",
    "\"\"\"\n",
    "🎯 주요 개선사항:\n",
    "1. 🔧 25+ 새로운 고급 특성 추가\n",
    "2. 🤖 3개 → 6개 모델로 다양성 확대\n",
    "3. 🏗️ 4가지 메타 러너 중 최적 선택\n",
    "4. 🎯 특성 선택 최적화 (SelectKBest)\n",
    "5. ⚙️ 하이퍼파라미터 정밀 튜닝\n",
    "6. 📊 다층 스태킹 구현\n",
    "\n",
    "🚀 예상 성능: 0.05739 → 0.045-0.055\n",
    "💪 목표: 0.04대 돌파!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80acb60b-fe0f-49e7-9236-ef89fb4c57e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11893428,
     "sourceId": 91716,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e58744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Importing Essential Libraries\n",
    "import pandas as pd  # ë°ì´í„° ì¡°ì‘ ë° ë¶„ì„ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import numpy as np  # ìˆ˜ì¹˜ ê³„ì‚°ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import re  # ì •ê·œí‘œí˜„ì‹ ì²˜ë¦¬ì— ì‚¬ìš© (ì˜ˆ: ë²ˆí˜¸íŒ íŒŒì‹± ë“±)\n",
    "import sys  # ì‹œìŠ¤í…œ ê´€ë ¨ ê¸°ëŠ¥ ì œê³µ (ì˜ˆ: ê²½ë¡œ ì„¤ì • ë“±)\n",
    "import os  # ìš´ì˜ì²´ì œì™€ì˜ ìƒí˜¸ì‘ìš© (ì˜ˆ: íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬ ë“±)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler  # íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ ë„êµ¬\n",
    "from sklearn.impute import SimpleImputer  # ê²°ì¸¡ê°’ ì²˜ë¦¬ ë„êµ¬\n",
    "from sklearn.model_selection import KFold, StratifiedKFold  # êµì°¨ê²€ì¦ ì „ëµ\n",
    "from sklearn.ensemble import GradientBoostingRegressor  # ì•™ìƒë¸” ê¸°ë°˜ íšŒê·€ ëª¨ë¸\n",
    "from sklearn.linear_model import Ridge  # ë¦¿ì§€ íšŒê·€ ëª¨ë¸ (ì„ í˜• ëª¨ë¸)\n",
    "from sklearn.metrics import mean_squared_error  # íšŒê·€ í‰ê°€ ì§€í‘œ (í‰ê· ì œê³±ì˜¤ì°¨)\n",
    "# 1. Data Loading and Initial Preparation\n",
    "# 0. Importing Essential Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Scikit-learn for preprocessing and model selection utilities\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1. ê²½ë¡œ ì„¤ì • (JupyterLab ë°©ì‹)\n",
    "base_path = Path('/Users/jangyeeun/Desktop/Kaggle01/russian-car-plates-prices-prediction/train.csv') \n",
    "train_path = base_path / 'train.csv'\n",
    "test_path = base_path / 'test.csv'\n",
    "supplemental_path = base_path / 'supplemental_english.py'\n",
    "\n",
    "# 2. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "print(\"\\në°ì´í„° ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘\")\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)\n",
    "print(f\"Train data shape: {train.shape}\")\n",
    "print(f\"Test data shape: {test.shape}\")\n",
    "\n",
    "# 4. supplemental_english.py ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "sys.path.append(str(base_path))\n",
    "import supplemental_english as supp\n",
    "print(\"supplemental_english.py ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "# 5. Train/Test í•©ì¹˜ê¸°\n",
    "train['is_train'] = 1\n",
    "test['is_train'] = 0\n",
    "df = pd.concat([train, test], ignore_index=True)\n",
    "print(f\"Combined DataFrame shape: {df.shape}\")\n",
    "print(\"Trainê³¼ Test ë°ì´í„° ë³‘í•© ì™„ë£Œ\")\n",
    "\n",
    "df\n",
    "## <mark>ğŸ“Œ ì™œ Trainê³¼ Testë¥¼ í•©ì¹ ê¹Œ?</mark>\n",
    "\n",
    "# 1. ë™ì¼í•œ ì „ì²˜ë¦¬ ì ìš©\n",
    "# - `train`ê³¼ `test`ë¥¼ ë”°ë¡œ ì²˜ë¦¬í•˜ë©´, **ê°™ì€ ê°’ì¸ë°ë„ ë‹¤ë¥´ê²Œ ì¸ì½”ë”©ë˜ê±°ë‚˜**, **ìŠ¤ì¼€ì¼ ê¸°ì¤€ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ**\n",
    "# - ë”°ë¼ì„œ **í•©ì³ì„œ ì²˜ë¦¬ í›„, ë‚˜ì¤‘ì— ë‹¤ì‹œ ë¶„ë¦¬í•˜ëŠ” ê²Œ ê°€ì¥ ì•ˆì „**\n",
    "\n",
    "# 2. ì¼ê´€ëœ Feature ìƒì„±\n",
    "#   - `test`ê¹Œì§€ í¬í•¨í•´ì„œ ë§Œë“¤ë©´, **ë” ì¼ë°˜í™”ëœ íŠ¹ì§•**ì„ ì–»ì„ ìˆ˜ ìˆê¸° ë•Œë¬¸\n",
    "\n",
    "# 3. ì¶”ê°€ ì»¬ëŸ¼ì„ í¸í•˜ê²Œ ìƒì„±í•˜ê¸° ìœ„í•´\n",
    "# - íŒŒìƒë³€ìˆ˜ë¥¼ ë§Œë“¤ ë•Œ,`train`, `test` ë‚˜ëˆ ì„œ ì²˜ë¦¬í•˜ëŠ” ê²ƒë³´ë‹¤ **í•˜ë‚˜ë¡œ í•©ì³ì„œ ì²˜ë¦¬í•˜ë©´ ë” íš¨ìœ¨ì ì´ê¸° ë•Œë¬¸**\n",
    "# - Kaggleì— ì œì¶œí•  ë•ŒëŠ” íŒŒìƒë³€ìˆ˜ë¥¼ ì œì¶œí•˜ëŠ” ê²Œ ì•„ë‹ˆë¼ ê·¸ê±¸ ê¸°ë°˜ìœ¼ë¡œ ë§Œë“  ì˜ˆì¸¡ ê²°ê³¼ë§Œ ì œì¶œí•˜ë©´ ë˜ê¸° ë•Œë¬¸ì— ê´œì°®ìŒ.\n",
    "\n",
    "# >ğŸ’¡ 'is_train' ì»¬ëŸ¼ì€ ì™œ ì¶”ê°€í•˜ëŠ”ì§€?\n",
    "# > ë‚˜ì¤‘ì— ë‹¤ì‹œ trainê³¼ testë¡œ êµ¬ë¶„í•˜ê¸° ìœ„í•´\n",
    "# 2. Plate Component Extraction and Basic Feature Engineering\n",
    "# ìë™ì°¨ ë²ˆí˜¸íŒ ë¬¸ìì—´ì—ì„œ êµ¬ì„± ìš”ì†Œ ì¶”ì¶œ í•¨ìˆ˜ ì •ì˜  \n",
    "# ëŸ¬ì‹œì•„ ì°¨ëŸ‰ ë²ˆí˜¸íŒ í˜•ì‹: ë¬¸ì-3ìë¦¬ ìˆ«ì-2ìë¦¬ ë¬¸ì-ì§€ì—­ ì½”ë“œ  \n",
    "def extract_components(plate):\n",
    "    \"\"\"\n",
    "    ì°¨ëŸ‰ ë²ˆí˜¸íŒ ë¬¸ìì—´ì—ì„œ ì²« ë¬¸ì, 3ìë¦¬ ìˆ«ì, ë§ˆì§€ë§‰ ë‘ ë¬¸ì, ì§€ì—­ ì½”ë“œ, ì „ì²´ ë¬¸ì ì¡°í•©ì„ ì¶”ì¶œ\n",
    "    \"\"\"\n",
    "    match = re.match(r'^([A-Z])(\\d{3})([A-Z]{2})(\\d{2,3})$', plate)\n",
    "    if match:\n",
    "        first_letter = match.group(1)       # ì²« ë²ˆì§¸ ë¬¸ì\n",
    "        number = match.group(2)             # 3ìë¦¬ ìˆ«ì\n",
    "        last_letters = match.group(3)       # ë§ˆì§€ë§‰ ë‘ ë¬¸ì\n",
    "        region_code = match.group(4)        # ì§€ì—­ ì½”ë“œ (2~3ìë¦¬)\n",
    "        full_letters = first_letter + last_letters  # ì „ì²´ ë¬¸ì ì¡°í•©\n",
    "        return first_letter, number, last_letters, region_code, full_letters\n",
    "    return None, None, None, None, None\n",
    "\n",
    "# ë²ˆí˜¸íŒ êµ¬ì„± ìš”ì†Œ ì¶”ì¶œí•˜ì—¬ ìƒˆë¡œìš´ ì»¬ëŸ¼ ìƒì„±\n",
    "print(\"ë²ˆí˜¸íŒ êµ¬ì„± ìš”ì†Œ ì¶”ì¶œ ì¤‘...\")\n",
    "df[['pre_lettre', 'numero', 'second_lettre', 'code_region', 'lettre_complet']] = \\\n",
    "    pd.DataFrame(df['plate'].apply(extract_components).tolist(), index=df.index)\n",
    "print(\"ë²ˆí˜¸íŒ êµ¬ì„± ìš”ì†Œ ì¶”ì¶œ ì™„ë£Œ\")\n",
    "df\n",
    "## <mark> ğŸ’™ plate - ë¬¸ì ê¸°ë°˜ íŒŒìƒ ì»¬ëŸ¼</mark>\n",
    "\n",
    "# | ì»¬ëŸ¼ëª…            | ì˜ë¯¸                         |\n",
    "# |------------------|------------------------------|\n",
    "# | `pre_lettre`     | ë²ˆí˜¸íŒ ì•ë¶€ë¶„ì˜ **ì²« ë¬¸ì**      |\n",
    "# | `numero`         | ë¬¸ì ì‚¬ì´ì— ìˆëŠ” **ìˆ«ì**        |\n",
    "# | `second_lettre`  | ë²ˆí˜¸íŒ ë’·ë¶€ë¶„ì˜ **ë§ˆì§€ë§‰ ë¬¸ì**   |\n",
    "# | `code_region`    | ë²ˆí˜¸íŒì— í‘œì‹œëœ **ì§€ì—­ ì½”ë“œ**     |\n",
    "# | `lettre_complet` | ë²ˆí˜¸íŒì˜ ì „ì²´ **ë¬¸ì ì¡°í•©**       |\n",
    "# 'date' ì»¬ëŸ¼ì„ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "print(\"'date' ì»¬ëŸ¼ì„ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ì™„ë£Œ.\")\n",
    "\n",
    "# ë‚ ì§œì—ì„œ ë‹¤ì–‘í•œ ì‹œê°„ ê´€ë ¨ íŠ¹ì„± ì¶”ì¶œ í•¨ìˆ˜\n",
    "def enrich_date_features(df):\n",
    "    \"\"\"\n",
    "    ë‚ ì§œ(datetime) ì»¬ëŸ¼ì—ì„œ ë‹¤ì–‘í•œ ì‹œê°„ ê¸°ë°˜ íŠ¹ì„±ê³¼ ì£¼ê¸°ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ë³€ìˆ˜ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    # ê¸°ë³¸ ì‹œê°„ íŠ¹ì„± ì¶”ì¶œ: ì—°ë„, ì›”, ì¼, ìš”ì¼, ì—°ì¤‘ ì£¼ì°¨, ë¶„ê¸°, ê¸°ì¤€ì¼ë¡œë¶€í„°ì˜ ì¼ìˆ˜, ì£¼ë§ ì—¬ë¶€, ìš”ì¼ ì´ë¦„\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df['week_of_year'] = df['date'].dt.isocalendar().week.astype(int)\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['total_days'] = (df['date'] - df['date'].min()).dt.days\n",
    "    df['is_weekend'] = df['date'].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "    df['day_name'] = df['date'].dt.day_name()\n",
    "\n",
    "    # ì£¼ê¸°ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” íŠ¹ì„± ì¶”ê°€ (ì‚¬ì¸/ì½”ì‚¬ì¸ ë³€í™˜): ìš”ì¼, ì¼, ì›”\n",
    "    # ëª¨ë¸ì´ ì‹œê°„ ì£¼ê¸°ë¥¼ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤Œ\n",
    "    df['weekday_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['weekday_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['day'] / 31)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['day'] / 31)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    return df\n",
    "\n",
    "# ë‚ ì§œ ê´€ë ¨ íŠ¹ì„± ì¶”ê°€ ìˆ˜í–‰\n",
    "df = enrich_date_features(df)\n",
    "print(\"ë‚ ì§œ íŠ¹ì„±ì„ ê¸°ë³¸ + ì£¼ê¸°ì  íŠ¹ì„±ìœ¼ë¡œ í™•ì¥ ì™„ë£Œ.\")\n",
    "df\n",
    "df.columns\n",
    "## <mark> ğŸ’™ date ê¸°ë°˜ íŒŒìƒ ì»¬ëŸ¼ </mark>\n",
    "\n",
    "| ì»¬ëŸ¼ëª…           | ì„¤ëª…                                                    |\n",
    "|------------------|---------------------------------------------------------|\n",
    "| `year`           | ë‚ ì§œì˜ **ì—°ë„** (ì˜ˆ: 2024)                              |\n",
    "| `month`          | ë‚ ì§œì˜ **ì›”** (1~12)                                    |\n",
    "| `day`            | ë‚ ì§œì˜ **ì¼** (1~31)                                    |\n",
    "| `day_of_week`    | **ìš”ì¼ ìˆ«ì** (ì›”:0 ~ ì¼:6)                              |\n",
    "| `week_of_year`   | í•´ë‹¹ ë‚ ì§œê°€ **1ë…„ ì¤‘ ëª‡ ë²ˆì§¸ ì£¼**ì¸ì§€                   |\n",
    "| `quarter`        | í•´ë‹¹ ë‚ ì§œì˜ **ë¶„ê¸°** (1~4ë¶„ê¸°)                           |\n",
    "| `total_days`     | ê¸°ì¤€ì¼ë¡œë¶€í„°ì˜ **ëˆ„ì  ì¼ìˆ˜** ë˜ëŠ” ë‚ ì§œ ê°„ê²©              |\n",
    "| `is_weekend`     | **ì£¼ë§ ì—¬ë¶€** (í† /ì¼ì´ë©´ 1, í‰ì¼ì´ë©´ 0)                 |\n",
    "| `day_name`       | ìš”ì¼ ì´ë¦„ (ì˜ˆ: Monday, Tuesday ë“±)                      |\n",
    "| `weekday_sin`    | ìš”ì¼ ì •ë³´ë¥¼ **ì‚¬ì¸ í•¨ìˆ˜**ë¡œ ë³€í™˜í•œ ê°’ (ì£¼ê¸°ì„± ë³´ì¡´ìš©)    |\n",
    "| `weekday_cos`    | ìš”ì¼ ì •ë³´ë¥¼ **ì½”ì‚¬ì¸ í•¨ìˆ˜**ë¡œ ë³€í™˜í•œ ê°’                 |\n",
    "| `day_sin`        | ë‚ ì§œ(1~31)ë¥¼ **ì‚¬ì¸ í•¨ìˆ˜**ë¡œ ë³€í™˜í•œ ê°’                  |\n",
    "| `day_cos`        | ë‚ ì§œ(1~31)ë¥¼ **ì½”ì‚¬ì¸ í•¨ìˆ˜**ë¡œ ë³€í™˜í•œ ê°’                |\n",
    "| `month_sin`      | ì›”(1~12)ì„ **ì‚¬ì¸ í•¨ìˆ˜**ë¡œ ë³€í™˜í•œ ê°’                    |\n",
    "| `month_cos`      | ì›”(1~12)ì„ **ì½”ì‚¬ì¸ í•¨ìˆ˜**ë¡œ ë³€í™˜í•œ ê°’                  |\n",
    "## <mark>ğŸ“Œ ì™œ ì‚¬ì¸Â·ì½”ì‚¬ì¸ ë³€í™˜ì„ ì‚¬ìš©í• ê¹Œ?</mark>\n",
    ">- ì‚¬ì¸(sin)Â·ì½”ì‚¬ì¸(cos) ë³€í™˜ì€  <b>ë‚ ì§œë‚˜ ì‹œê°„ì²˜ëŸ¼ ì£¼ê¸°ì„±(cyclical) </b>ì´ ìˆëŠ” ë°ì´í„°ë¥¼ ìˆ˜ì¹˜í™”í•  ë•Œ ì‚¬ìš©ë˜ëŠ” ê¸°ë²•\n",
    ">- ì¼ë°˜ì ì¸ ìˆ«ìì²˜ëŸ¼ ë‹¤ë£¨ë©´ ëª¨ë¸ì´ ì£¼ê¸°ì„±ì„ ì œëŒ€ë¡œ ì¸ì‹í•˜ì§€ ëª»í•´ì„œ ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì´ë¥¼ í•´ê²°í•˜ë ¤ê³  ì‚¬ìš©\n",
    ">- 2 * np.pi * df['day_of_week'] / 7  ì€ <b>ìš”ì¼ ìˆ«ì(0~6)</b>ë¥¼ <b>ì›ì˜ ê°ë„(ë¼ë””ì•ˆ)</b>ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •\n",
    ">- ì˜ˆì‹œ\n",
    "| ìš”ì¼ | ìš”ì¼ ì´ë¦„ | Î¸ (ë¼ë””ì•ˆ)           | cos(Î¸) | sin(Î¸) | ì¢Œí‘œ (x, y)        |\n",
    "| -- | ----- | ----------------- | ------ | ------ | ---------------- |\n",
    "| 0  | ì›”ìš”ì¼   | $0$               | 1.000  | 0.000  | (1.000, 0.000)   |\n",
    "| 1  | í™”ìš”ì¼   | $\\frac{2\\pi}{7}$  | 0.623  | 0.782  | (0.623, 0.782)   |\n",
    "| 2  | ìˆ˜ìš”ì¼   | $\\frac{4\\pi}{7}$  | -0.222 | 0.975  | (-0.222, 0.975)  |\n",
    "| 3  | ëª©ìš”ì¼   | $\\frac{6\\pi}{7}$  | -0.901 | 0.434  | (-0.901, 0.434)  |\n",
    "| 4  | ê¸ˆìš”ì¼   | $\\frac{8\\pi}{7}$  | -0.901 | -0.434 | (-0.901, -0.434) |\n",
    "| 5  | í† ìš”ì¼   | $\\frac{10\\pi}{7}$ | -0.222 | -0.975 | (-0.222, -0.975) |\n",
    "| 6  | ì¼ìš”ì¼   | $\\frac{12\\pi}{7}$ | 0.623  | -0.782 | (0.623, -0.782)  |\n",
    "\n",
    "\n",
    ">- sinê³¼ cosë¥¼ ë‘˜ ë‹¤ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” ì£¼ê¸°ì  íŠ¹ì„±ì„ 2ì°¨ì› ì¢Œí‘œê³„ì— ë‚˜íƒ€ë‚´ê¸° ìœ„í•¨(í•˜ë‚˜ë§Œ ì“°ë©´ ì •ë³´ê°€ ëª¨ìë¼ì„œ ì£¼ê¸°ì„±ì„ ì™„ì „íˆ í‘œí˜„í•  ìˆ˜ ì—†ìŒ)\n",
    ">- cos(ê°ë„)ëŠ” xì¢Œí‘œ,sin(ê°ë„)ëŠ” yì¢Œí‘œ\n",
    ">- ì˜ˆì‹œ\n",
    "ì›”ìš”ì¼ = 0 ì¼ ë•Œ,2Ï€â‹…0/7=0 ë”°ë¼ì„œ, cos(0)=1,sin(0)=0, ì¢Œí‘œ(1,0)\n",
    ">- (x,y)=(cos(Î¸),sin(Î¸))\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=\"sincos.png\", width=400))\n",
    "# 3. Extracting Information from Supplemental Data\n",
    "# supplemental_english.py íŒŒì¼ì—ì„œ REGION_CODESë¥¼ ëª…ì‹œì ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "# REGION_CODES ë”•ì…”ë„ˆë¦¬ êµ¬ì¡°ì— í”„ë¡œê·¸ë˜ë°ì ìœ¼ë¡œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ í•¨\n",
    "file_path = '/Users/jangyeeun/Desktop/ìºê¸€/russian-car-plates-prices-prediction/supplemental_english.py'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    python_content = file.readlines()\n",
    "\n",
    "# REGION_CODES ë”•ì…”ë„ˆë¦¬ ì¶”ì¶œ\n",
    "# ì´ ë¡œì§ì€ Python íŒŒì¼ì—ì„œ REGION_CODES ë³€ìˆ˜ë¥¼ íŒŒì‹±í•¨\n",
    "# íŒŒì‹± : ë³µì¡í•œ í…ìŠ¤íŠ¸(ë¬¸ìì—´)ë¥¼ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ í•´ì„í•´ì„œ í•„ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ê³¼ì •\n",
    "region_start = [i for i, line in enumerate(python_content) if \"REGION_CODES = {\" in line][0]\n",
    "bracket_count = 0\n",
    "region_end = None\n",
    "\n",
    "for i in range(region_start, len(python_content)):\n",
    "    line = python_content[i]\n",
    "    bracket_count += line.count(\"{\") - line.count(\"}\")  # ì¤‘ê´„í˜¸ ì—´ê¸°/ë‹«ê¸° ê°œìˆ˜ ì¶”ì \n",
    "    if bracket_count == 0:\n",
    "        region_end = i\n",
    "        break\n",
    "region_lines = python_content[region_start:region_end+1]\n",
    "\n",
    "# ë”•ì…”ë„ˆë¦¬ë¥¼ í‰íƒ„í™”í•˜ì—¬ ì§€ì—­ ì½”ë“œì— ëŒ€í•œ DataFrame ìƒì„±\n",
    "# í‰íƒ„í™”(flattening): ì¤‘ì²©ëœ êµ¬ì¡°(ë”•ì…”ë„ˆë¦¬ ë‚´ë¶€ì˜ ë¦¬ìŠ¤íŠ¸)ë¥¼ 1ì°¨ì› ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì •ë¦¬í•˜ëŠ” ì‘ì—…\n",
    "\n",
    "region_codes = []\n",
    "for line in region_lines[1:]:  # 'REGION_CODES = {' ë¼ì¸ì€ ê±´ë„ˆëœ€\n",
    "    if \":\" in line:\n",
    "        key, value = map(str.strip, line.split(\":\", 1))  # ì§€ì—­ëª…(key)ê³¼ ì§€ì—­ ì½”ë“œ ë¦¬ìŠ¤íŠ¸(value)ë¥¼ ë¬¸ìì—´ë¡œ ë¶„ë¦¬í•˜ê³  ê³µë°± ì œê±°\n",
    "        value = value.rstrip(\",\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"\\\"\", \"\").split(\",\")  # ë¦¬ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ ê¸°í˜¸ ì œê±° í›„ ë¶„í• \n",
    "        region_codes.extend([(key.strip('\"'), code.strip()) for code in value if code.strip()])  # ì§€ì—­ëª…ê³¼ ì½”ë“œ ìŒì„ íŠœí”Œë¡œ ì €ì¥, ë¹ˆ ë¬¸ìì—´ ì œì™¸\n",
    "\n",
    "# ëª¨ë“  ì§€ì—­ëª…ê³¼ ì§€ì—­ì½”ë“œë¥¼ íŠœí”Œ í˜•íƒœë¡œ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥í•œ ê²ƒì„ DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "region_codes_df = pd.DataFrame(region_codes, columns=[\"region_name\", \"region_code\"])\n",
    "region_codes_df['region_code'] = region_codes_df['region_code'].str.strip()  # ì½”ë“œ ê°’ì˜ ì•ë’¤ ê³µë°± ì œê±°\n",
    "print(\"supplemental íŒŒì¼ì—ì„œ REGION_CODES íŒŒì‹± ì™„ë£Œ.\")\n",
    "\n",
    "\n",
    "# 'code_region' ê¸°ì¤€ìœ¼ë¡œ ì›ë³¸ ë°ì´í„°í”„ë ˆì„ê³¼ ì§€ì—­ëª…ì„ ë³‘í•©\n",
    "df = df.merge(region_codes_df, left_on='code_region', right_on='region_code', how='left')\n",
    "# ë³‘í•© í›„ ì¤‘ë³µëœ 'region_code' ì»¬ëŸ¼ ì œê±°\n",
    "df.drop(columns=['region_code'], inplace=True)\n",
    "print(\"ì§€ì—­ ì½”ë“œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì§€ì—­ëª… ë³‘í•© ì™„ë£Œ.\")\n",
    "\n",
    "df.columns\n",
    "\n",
    "## ğŸ’™ íŒŒìƒí”¼ì²˜ ì¤‘ê°„ ì •ë¦¬\n",
    ">- ê¸°ë³¸ ì»¬ëŸ¼ : 'id', 'plate', 'date', 'price'\n",
    ">- íŠ¸ë ˆì´ë‹/í…ŒìŠ¤íŠ¸ì…‹ êµ¬ë¶„ : 'is_train'\n",
    ">- plate - ë¬¸ì : 'pre_lettre', 'numero','second_lettre', 'code_region', 'lettre_complet'\n",
    ">- date : 'year', 'month',\n",
    "       'day', 'day_of_week', 'week_of_year', 'quarter', 'total_days',\n",
    "       'is_weekend', 'day_name', 'weekday_sin', 'weekday_cos', 'day_sin',\n",
    "       'day_cos', 'month_sin', 'month_cos'\n",
    ">- ì§€ì—­ì½”ë“œ - ì§€ì—­ ì´ë¦„ : 'region_name'\n",
    "# 'supp' ëª¨ë“ˆì˜ REGION_CODESë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ìˆ«ì ì½”ë“œ ìƒì„±\n",
    "\n",
    "def get_region_code_numeric(row):\n",
    "    \"\"\"region codeë¥¼ REGION_CODES ë”•ì…”ë„ˆë¦¬ì—ì„œ ì²« ë²ˆì§¸ ê°’ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ìˆ«ìí™”\"\"\"\n",
    "    code_region = str(row['code_region'])  # ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì¡°íšŒ\n",
    "    for region, codes in supp.REGION_CODES.items():\n",
    "        if code_region in codes:\n",
    "            return int(codes[0])  # í•´ë‹¹ ì§€ì—­ì˜ ì²« ë²ˆì§¸ ì½”ë“œ ë°˜í™˜\n",
    "    return -1  # í•´ë‹¹ ì—†ìŒ ì‹œ -1 ë°˜í™˜\n",
    "\n",
    "df['region_code_numeric'] = df.apply(get_region_code_numeric, axis=1)\n",
    "print(\"'region_code_numeric' íŠ¹ì„± ìƒì„± ì™„ë£Œ.\")\n",
    "# ì •ë¶€ ê´€ë ¨ ì •ë³´ ì»¬ëŸ¼ ì´ˆê¸°í™”\n",
    "df['is_government'] = 0\n",
    "df['government_agency'] = None\n",
    "df['forbidden_to_buy'] = False\n",
    "df['road_advantage'] = False\n",
    "df['significance_level'] = 0\n",
    "\n",
    "# 'GOVERNMENT_CODES'ë¥¼ í™œìš©í•œ ì •ë¶€ ì°¨ëŸ‰ ë²ˆí˜¸íŒ ì •ë³´ ì¶”ì¶œ í•¨ìˆ˜\n",
    "def get_government_info(row):\n",
    "    \"\"\"\n",
    "    ë²ˆí˜¸íŒ êµ¬ì„± ìš”ì†Œì™€ GOVERNMENT_CODES ë”•ì…”ë„ˆë¦¬ë¥¼ ì´ìš©í•´ ì •ë¶€ ì°¨ëŸ‰ ì—¬ë¶€ ë° ì •ë³´ë¥¼ ë°˜í™˜\n",
    "    \"\"\"\n",
    "    # ê²°ì¸¡ê°’ì´ ìˆëŠ” ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬\n",
    "    if pd.isna(row['pre_lettre']) or pd.isna(row['numero']) or pd.isna(row['code_region']):\n",
    "        return 0, None, False, False, 0\n",
    "\n",
    "    first_letter = row['pre_lettre']\n",
    "    numbers = int(row['numero']) if pd.notna(row['numero']) else -1\n",
    "    region_code = row['code_region']\n",
    "\n",
    "    # ì •ì˜ëœ ì •ë¶€ ì°¨ëŸ‰ ì½”ë“œ íƒìƒ‰\n",
    "    for (letters, (start, end), code), (agency, forbidden, advantage, significance) in supp.GOVERNMENT_CODES.items():\n",
    "        # ë²ˆí˜¸íŒì´ ì •ë¶€ ì°¨ëŸ‰ íŒ¨í„´ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸\n",
    "        if first_letter == letters[0] and region_code == code and start <= numbers <= end:\n",
    "            return 1, agency, bool(forbidden), bool(advantage), significance\n",
    "\n",
    "    return 0, None, False, False, 0  # í•´ë‹¹ ì—†ìŒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜\n",
    "\n",
    "# ì •ë¶€ ì°¨ëŸ‰ ì •ë³´ë¥¼ ê° í–‰ì— ì ìš©í•˜ì—¬ ìƒˆë¡œìš´ íŠ¹ì„± ìƒì„±\n",
    "print(\"ì •ë¶€ ì°¨ëŸ‰ ë²ˆí˜¸íŒ ì •ë³´ ì¶”ì¶œ ì¤‘...\")\n",
    "govt_info = df.apply(get_government_info, axis=1)\n",
    "df['is_government'] = [info[0] for info in govt_info]\n",
    "df['government_agency'] = [info[1] for info in govt_info]\n",
    "df['forbidden_to_buy'] = [info[2] for info in govt_info]\n",
    "df['road_advantage'] = [info[3] for info in govt_info]\n",
    "df['significance_level'] = [info[4] for info in govt_info]\n",
    "print(\"ì •ë¶€ ì°¨ëŸ‰ ê´€ë ¨ íŠ¹ì„± ìƒì„± ì™„ë£Œ.\")\n",
    "\n",
    "# ì—´ ì´ë¦„ ë³€ê²½: ê°€ë…ì„± ë° ì¼ê´€ì„± í–¥ìƒ\n",
    "df.rename(columns={\n",
    "    'pre_lettre': 'first_letter',\n",
    "    'second_lettre': 'last_letters',\n",
    "    'lettre_complet': 'full_letters',\n",
    "    'numero': 'numbers',\n",
    "    'code_region': 'region_code_original'  # 'region_code_numeric'ì™€ í˜¼ë™ ë°©ì§€ë¥¼ ìœ„í•´ ì´ë¦„ ë³€ê²½\n",
    "}, inplace=True)\n",
    "print(\"ì—´ ì´ë¦„ ë³€ê²½ ì™„ë£Œ.\")\n",
    "\n",
    "# 'numbers' ì»¬ëŸ¼ì„ ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜ (ì—ëŸ¬ëŠ” NaNìœ¼ë¡œ ì²˜ë¦¬ í›„ 0ìœ¼ë¡œ ëŒ€ì²´)\n",
    "# ì´í›„ ìˆ˜ì¹˜ ë¹„êµ ë° ê³„ì‚°ì„ ìœ„í•´ í•„ìˆ˜\n",
    "df['numbers'] = pd.to_numeric(df['numbers'], errors='coerce').fillna(0).astype(int)\n",
    "print(\"'numbers' ì»¬ëŸ¼ ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜ ì™„ë£Œ.\")\n",
    "#Frequency Encoding for the 'numbers' feature\n",
    "# This replaces the number with its frequency of occurrence in the dataset.\n",
    "freq_table = df['numbers'].value_counts().reset_index()\n",
    "freq_table.columns = ['numbers', 'n']\n",
    "freq_table['freq_enc'] = freq_table['n'] / freq_table['n'].sum()\n",
    "freq_table['log_freq_enc'] = np.log1p(freq_table['freq_enc']) # Log transform for potential skewed distribution\n",
    "\n",
    "# Merge frequency encodings back to the main DataFrame\n",
    "df = df.merge(freq_table[['numbers', 'freq_enc', 'log_freq_enc']], \n",
    "              on='numbers', how='left')\n",
    "df.rename(columns={'freq_enc': 'numbers_freq_enc', \n",
    "                  'log_freq_enc': 'numbers_log_freq_enc'}, inplace=True)\n",
    "print(\"Applied Frequency Encoding to 'numbers' feature.\")\n",
    "\n",
    "# Target Encoding (Mean Encoding) for categorical features\n",
    "# This technique replaces a categorical value with the mean of the target variable\n",
    "# for that category. It's crucial to perform this only on the training data\n",
    "# to avoid data leakage from the test set.\n",
    "train_data = df[df['is_train'] == 1].copy()\n",
    "\n",
    "# For regions: calculate mean price for each region name\n",
    "region_mean_price = train_data.groupby('region_name')['price'].mean().reset_index()\n",
    "region_mean_price.columns = ['region_name', 'region_mean_price']\n",
    "df = df.merge(region_mean_price, on='region_name', how='left')\n",
    "print(\"Target encoded 'region_name' with 'region_mean_price'.\")\n",
    "\n",
    "# For first letter: calculate mean price for each first letter\n",
    "first_letter_mean_price = train_data.groupby('first_letter')['price'].mean().reset_index()\n",
    "first_letter_mean_price.columns = ['first_letter', 'first_letter_mean_price']\n",
    "df = df.merge(first_letter_mean_price, on='first_letter', how='left')\n",
    "print(\"Target encoded 'first_letter' with 'first_letter_mean_price'.\")\n",
    "\n",
    "# For last letters: calculate mean price for each last letters combination\n",
    "last_letters_mean_price = train_data.groupby('last_letters')['price'].mean().reset_index()\n",
    "last_letters_mean_price.columns = ['last_letters', 'last_letters_mean_price']\n",
    "df = df.merge(last_letters_mean_price, on='last_letters', how='left')\n",
    "print(\"Target encoded 'last_letters' with 'last_letters_mean_price'.\")\n",
    "\n",
    "# Logarithmic transformation of the target variable 'price'\n",
    "# This is a common practice in regression to make the target distribution more normal\n",
    "# and reduce the impact of outliers, improving model performance.\n",
    "df['log_price'] = np.log1p(df['price'])\n",
    "print(\"Applied logarithmic transformation (log1p) to 'price' to create 'log_price'.\")\n",
    "\n",
    "# --- Newly Added Features for enhanced modeling ---\n",
    "\n",
    "# Number Length and Uniqueness:\n",
    "df['number_length'] = df['numbers'].apply(lambda x: len(str(x))) # Length of the numeric part\n",
    "df['is_single_digit'] = (df['number_length'] == 1).astype(int) # Binary flag for single-digit numbers\n",
    "print(\"Added 'number_length' and 'is_single_digit' features.\")\n",
    "\n",
    "# Frequency of letter + region combinations:\n",
    "# This captures the popularity or rarity of specific plate patterns within regions.\n",
    "df['letters_region'] = df['full_letters'] + \"_\" + df['region_code_original'].astype(str)\n",
    "freq_lr = df['letters_region'].value_counts(normalize=True).to_dict()\n",
    "df['letters_region_freq'] = df['letters_region'].map(freq_lr)\n",
    "print(\"Calculated 'letters_region_freq' for letter-region combinations.\")\n",
    "\n",
    "# Relative Prestige Ranking:\n",
    "# Convert prestige score to a rank, normalized between 0 and 1.\n",
    "# This gives a relative measure of prestige across all plates.\n",
    "from scipy.stats import rankdata\n",
    "df['prestige_rank'] = rankdata(df['prestige_score'].astype(int), method='average') / len(df)\n",
    "print(\"Created 'prestige_rank' based on 'prestige_score'.\")\n",
    "\n",
    "# Interaction Features:\n",
    "df['letter_number_combo'] = df['full_letters'] + \"_\" + df['numbers'].astype(str)\n",
    "# Interaction between 'is_government' and 'prestige_score'\n",
    "df['is_gov_and_prestige'] = df['is_government'] * df['prestige_score'].astype(int)\n",
    "print(\"Added 'letter_number_combo' and 'is_gov_and_prestige' interaction features.\")\n",
    "\n",
    "# Similarity with Known Plates (Textual Embedding using CountVectorizer):\n",
    "# This attempts to capture patterns in letter sequences.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Using character n-grams to capture patterns like 'AA', 'AB', 'BA'\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(1,2))\n",
    "# Apply to 'full_letters' (e.g., 'XAA', 'TMM')\n",
    "letter_features = vectorizer.fit_transform(df['full_letters'].fillna(''))\n",
    "# Note: 'letter_features' is a sparse matrix and needs to be integrated into the\n",
    "# modeling pipeline if directly used. For now, it's generated for demonstration.\n",
    "print(f\"Generated textual features for 'full_letters' using CountVectorizer. Shape: {letter_features.shape}\")\n",
    "\n",
    "# Finer Geography:\n",
    "# Flag common premium regions (e.g., major cities/oblasts) as a binary feature.\n",
    "premium_regions = ['Moscow', 'Saint Petersburg', 'Moscow Oblast']\n",
    "df['is_premium_region'] = df['region_name'].isin(premium_regions).astype(int)\n",
    "print(\"Created 'is_premium_region' feature for major economic centers.\")\n",
    "\n",
    "# End of Feature Engineering section\n",
    "print(\"\\nFeature engineering complete. DataFrame is ready for model training.\")\n",
    "print(f\"Final DataFrame shape after feature engineering: {df.shape}\")\n",
    "# 4. Handling Missing Values and Categorizing Government Agencies\n",
    "# 'government_agency'ì˜ ê²°ì¸¡ê°’ì„ \"Non-governmental\"ë¡œ ëŒ€ì²´\n",
    "df['government_agency'] = df['government_agency'].fillna('Non-governmental')\n",
    "print(\"Missing 'government_agency' values filled with 'Non-governmental'.\")\n",
    "\n",
    "# ì •ë¶€ ê¸°ê´€ì„ ë³´ë‹¤ ì¼ë°˜ì ì¸ ê·¸ë£¹ìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” í•¨ìˆ˜\n",
    "# ì´ëŠ” 'government_agency' ì»¬ëŸ¼ì˜ ë†’ì€ ë²”ì£¼ ìˆ˜ë¥¼ ì¤„ì´ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.\n",
    "def categorize_agency(agency):\n",
    "    \"\"\"\n",
    "    íŠ¹ì • ì •ë¶€ ê¸°ê´€ëª…ì„ ë” ë„“ê³  ì¼ë°˜ì ì¸ ê·¸ë£¹ìœ¼ë¡œ ë¶„ë¥˜í•˜ì—¬\n",
    "    í”¼ì²˜ë¥¼ ë‹¨ìˆœí™”í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    if agency == 'Non-governmental':\n",
    "        return 'Non-governmental'\n",
    "    elif 'President' in agency:\n",
    "        return 'Presidential'\n",
    "    elif 'Police' in agency.lower() or 'Internal Affairs' in agency:\n",
    "        return 'Police/Security'\n",
    "    elif 'Government' in agency:\n",
    "        return 'Government'\n",
    "    elif 'Military' in agency or 'Army' in agency or 'Defense' in agency:\n",
    "        return 'Military'\n",
    "    elif 'Federal' in agency:\n",
    "        return 'Federal Services'\n",
    "    elif 'Judge' in agency or 'Court' in agency or 'Justice' in agency or 'prosecutor' in agency.lower():\n",
    "        return 'Judicial'\n",
    "    elif 'Administration' in agency:\n",
    "        return 'Administration'\n",
    "    else:\n",
    "        return 'Other Governmental'\n",
    "\n",
    "# ë¶„ë¥˜ í•¨ìˆ˜ë¥¼ ì ìš©í•´ ìƒˆë¡œìš´ 'agency_category' í”¼ì²˜ ìƒì„±\n",
    "df['agency_category'] = df['government_agency'].apply(categorize_agency)\n",
    "print(\"Government agencies categorized into broader groups.\")\n",
    "\n",
    "# ê° ê¸°ê´€ ê·¸ë£¹ì— ëŒ€í•´ ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ ì´ì§„ ë³€ìˆ˜ ìƒì„±\n",
    "# ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ëª¨ë¸ì— ì í•©í•œ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "agency_dummies = pd.get_dummies(df['agency_category'], prefix='agency')\n",
    "df = pd.concat([df, agency_dummies], axis=1)\n",
    "print(\"One-hot encoded 'agency_category' into binary features.\")\n",
    "\n",
    "# (í•™ìŠµ ë°ì´í„°ì— í•œí•´ì„œ) ê¸°ê´€ ê·¸ë£¹ë³„ í‰ê·  ê°€ê²©ì„ ê³„ì‚°í•´ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ\n",
    "if 'price' in df.columns:  # 'price' ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ í›„ ê³„ì‚°\n",
    "    agency_price = df[df['is_train'] == 1].groupby('agency_category')['price'].agg(['mean', 'count']).sort_values('mean', ascending=False)\n",
    "    print(\"\\nAverage price per agency category (Training Data):\")\n",
    "    print(agency_price)\n",
    "# 5. Advanced Plate Feature Creation\n",
    "# 'numbers' ì»¬ëŸ¼ì˜ ê²°ì¸¡ê°’ì´ ìˆì„ ê²½ìš° ëŒ€ë¹„í•˜ì—¬ ë‹¤ì‹œ 0ìœ¼ë¡œ ì±„ì›€\n",
    "df['numbers'] = df['numbers'].fillna(0)\n",
    "\n",
    "# 'full_letters'ì—ì„œ ë°˜ë³µë˜ëŠ” ë¬¸ìê°€ ìˆëŠ”ì§€ í™•ì¸ (ì˜ˆ: 'AAA', 'XXX')\n",
    "# ì´ëŸ° íŒ¨í„´ì€ ì„ í˜¸ë˜ëŠ” íŠ¹ì§•ì¼ ìˆ˜ ìˆìŒ\n",
    "df['has_repeated_letters'] = df['full_letters'].str.replace(r'(.)(?=.*\\1)', '', regex=True).str.len() < df['full_letters'].str.len()\n",
    "print(\"Created 'has_repeated_letters' feature.\")\n",
    "\n",
    "# 'numbers'ì—ì„œ ë°˜ë³µë˜ëŠ” ìˆ«ìê°€ ìˆëŠ”ì§€ í™•ì¸ (ì˜ˆ: '111', '777')\n",
    "# ì´ëŸ° ìˆ«ì íŒ¨í„´ì€ 'ì•„ë¦„ë‹¤ìš´' ë˜ëŠ” 'ëª…ì˜ˆë¡œìš´' ìˆ«ìë¡œ ê°„ì£¼ë¨\n",
    "df['has_repeated_numbers'] = df['numbers'].apply(\n",
    "    lambda n: bool(re.search(r'(\\d)\\1', f\"{int(n):03d}\")) # 3ìë¦¬ ìˆ«ì í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì˜ˆ: 7 -> 007)\n",
    ")\n",
    "print(\"Created 'has_repeated_numbers' feature.\")\n",
    "\n",
    "# ì—°ì†ëœ ìˆ«ìê°€ ìˆëŠ”ì§€ í™•ì¸ (ì˜ˆ: '123', '987')\n",
    "# ì´ ì—­ì‹œ ëª…ì˜ˆë¥¼ ë‚˜íƒ€ë‚´ëŠ” íŒ¨í„´ì¼ ìˆ˜ ìˆìŒ\n",
    "df['has_sequential_numbers'] = df['numbers'].apply(\n",
    "    lambda n: bool(re.search(r'123|234|345|456|567|678|789|987|876|765|654|543|432|321', f\"{int(n):03d}\"))\n",
    ")\n",
    "print(\"Created 'has_sequential_numbers' feature.\")\n",
    "\n",
    "# ê±°ìš¸ ìˆ«ì(ì˜ˆ: '121', '303') í˜¹ì€ íŒ°ë¦°ë“œë¡¬ ìˆ«ì(ì˜ˆ: '111')ì¸ì§€ í™•ì¸\n",
    "# ì´ëŸ° íŒ¨í„´ë„ íŠ¹ë³„í•œ ì˜ë¯¸ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆìŒ\n",
    "df['has_mirror_numbers'] = df['numbers'].apply(\n",
    "    lambda n: (str(int(n))[0] == str(int(n))[-1]) or (str(int(n)) == str(int(n))[::-1])\n",
    ")\n",
    "print(\"Created 'has_mirror_numbers' feature.\")\n",
    "\n",
    "# ëª…ì˜ˆë¡œìš´ ê¸€ì ì¡°í•© ë¦¬ìŠ¤íŠ¸ ì •ì˜ (ì˜ˆ: 'AAA', 'XXX' ë“± íŠ¹ì • ì¡°í•©)\n",
    "prestigious_letter_series = [\"AAA\", \"MMM\", \"EEE\", \"KKK\", \"OOO\", \"PPP\", \"CCC\", \"TTT\", \"XXX\"]\n",
    "df['is_beautiful_series'] = df['full_letters'].isin(prestigious_letter_series)\n",
    "print(\"Created 'is_beautiful_series' feature based on prestigious letter combinations.\")\n",
    "\n",
    "# ëª…ì˜ˆë¡œìš´ ìˆ«ì ì¡°í•© ë¦¬ìŠ¤íŠ¸ ì •ì˜ (ì˜ˆ: ë‹¨ì¼ ìˆ«ì, 3ìë¦¬ ë°˜ë³µ ìˆ«ì, 100 ë‹¨ìœ„ ë“±)\n",
    "prestigious_numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 111, 222, 333, 444, 555, 666, 777, 888, 999,\n",
    "                       100, 200, 300, 400, 500, 600, 700, 800, 900, 7]  # 7ì€ í–‰ìš´ì˜ ìˆ«ìë¡œ ìì£¼ ê°„ì£¼ë¨\n",
    "df['is_prestigious_number'] = df['numbers'].isin(prestigious_numbers)\n",
    "print(\"Created 'is_prestigious_number' feature based on specific prestigious number patterns.\")\n",
    "\n",
    "# ê¸€ìì˜ ë³µì¡ë„ë¥¼ ê³ ìœ  ë¬¸ì ìˆ˜ë¡œ ê³„ì‚°\n",
    "# ë³µì¡ë„ê°€ ë‚®ìœ¼ë©´ (ì˜ˆ: 'AAA') ë‹¨ìˆœí•˜ê³  ëª…ì˜ˆë¡œìš´ ê²ƒìœ¼ë¡œ íŒë‹¨ ê°€ëŠ¥\n",
    "df['letter_complexity'] = df['full_letters'].apply(\n",
    "    lambda x: len(set(x)) if pd.notnull(x) else 0\n",
    ")\n",
    "print(\"Calculated 'letter_complexity' feature.\")\n",
    "\n",
    "# ì—¬ëŸ¬ ëª…ì˜ˆ ê´€ë ¨ í”¼ì²˜ë¥¼ ê°€ì¤‘í•©í•˜ì—¬ ì¢…í•©ì ì¸ ëª…ì˜ˆ ì ìˆ˜ ìƒì„±\n",
    "# ë‹¤ì–‘í•œ ì‹ í˜¸ë¥¼ í•˜ë‚˜ì˜ ìˆ«ì ì ìˆ˜ë¡œ í†µí•©í•¨\n",
    "df['prestige_score'] = (\n",
    "    (df['is_beautiful_series'].astype(int) * 3) +  # ì•„ë¦„ë‹¤ìš´ ê¸€ì ì¡°í•©ì— ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬\n",
    "    (df['is_prestigious_number'].astype(int) * 2) +  # ëª…ì˜ˆë¡œìš´ ìˆ«ìì— ì¤‘ê°„ ê°€ì¤‘ì¹˜ ë¶€ì—¬\n",
    "    (df['has_repeated_letters'].astype(int) * 1) +\n",
    "    (df['has_repeated_numbers'].astype(int) * 1) +\n",
    "    (df['has_sequential_numbers'].astype(int) * 1) +\n",
    "    (df['has_mirror_numbers'].astype(int) * 1) +\n",
    "    (df['significance_level'].fillna(0))  # ì •ë¶€ ì¤‘ìš”ë„ ì ìˆ˜ í¬í•¨ (ê²°ì¸¡ì¹˜ëŠ” 0ìœ¼ë¡œ ì²˜ë¦¬)\n",
    ")\n",
    "print(\"Calculated 'prestige_score' by combining various prestige indicators.\")\n",
    "\n",
    "# 'prestige_score'ë¥¼ ë²”ì£¼í˜• íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ëª¨ë¸ ë˜ëŠ” ì‹œê°í™”ì— í™œìš© ê°€ëŠ¥í•˜ê²Œ í•¨\n",
    "df['prestige_score'] = df['prestige_score'].astype('category')\n",
    "print(\"Converted 'prestige_score' to categorical type.\")\n",
    "# 6. Encoding Categorical Variables and Advanced Feature Interactions\n",
    "\n",
    "\n",
    "# 'numbers' í”¼ì²˜ì— ëŒ€í•´ ë¹ˆë„ ì¸ì½”ë”© ìˆ˜í–‰\n",
    "# ë°ì´í„°ì…‹ ë‚´ì—ì„œ ìˆ«ìì˜ ë°œìƒ ë¹ˆë„ë¡œ ìˆ«ìë¥¼ ëŒ€ì²´í•¨\n",
    "freq_table = df['numbers'].value_counts().reset_index()\n",
    "freq_table.columns = ['numbers', 'n']\n",
    "freq_table['freq_enc'] = freq_table['n'] / freq_table['n'].sum()\n",
    "freq_table['log_freq_enc'] = np.log1p(freq_table['freq_enc']) # ë¶„í¬ ì™œê³¡ ê°€ëŠ¥ì„±ì„ ì¤„ì´ê¸° ìœ„í•œ ë¡œê·¸ ë³€í™˜\n",
    "\n",
    "# ë¹ˆë„ ì¸ì½”ë”© ê²°ê³¼ë¥¼ ì›ë³¸ DataFrameì— ë³‘í•©\n",
    "df = df.merge(freq_table[['numbers', 'freq_enc', 'log_freq_enc']], \n",
    "              on='numbers', how='left')\n",
    "df.rename(columns={'freq_enc': 'numbers_freq_enc', \n",
    "                  'log_freq_enc': 'numbers_log_freq_enc'}, inplace=True)\n",
    "print(\"â€˜numbersâ€™ í”¼ì²˜ì— ë¹ˆë„ ì¸ì½”ë”© ì ìš© ì™„ë£Œ.\")\n",
    "\n",
    "# ë²”ì£¼í˜• í”¼ì²˜ì— ëŒ€í•´ íƒ€ê²Ÿ ì¸ì½”ë”©(í‰ê·  ì¸ì½”ë”©) ìˆ˜í–‰\n",
    "# ê° ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ íƒ€ê²Ÿ ë³€ìˆ˜ì˜ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´í•˜ëŠ” ê¸°ë²•.\n",
    "# ë°ì´í„° ëˆ„ìˆ˜ë¥¼ ë§‰ê¸° ìœ„í•´ ë°˜ë“œì‹œ í•™ìŠµ ë°ì´í„°ì—ë§Œ ì ìš©í•´ì•¼ í•¨.\n",
    "train_data = df[df['is_train'] == 1].copy()\n",
    "\n",
    "# ì§€ì—­ë³„ í‰ê·  ê°€ê²© ê³„ì‚°\n",
    "region_mean_price = train_data.groupby('region_name')['price'].mean().reset_index()\n",
    "region_mean_price.columns = ['region_name', 'region_mean_price']\n",
    "df = df.merge(region_mean_price, on='region_name', how='left')\n",
    "print(\"â€˜region_nameâ€™ì— ëŒ€í•´ íƒ€ê²Ÿ ì¸ì½”ë”©(â€˜region_mean_priceâ€™) ì ìš© ì™„ë£Œ.\")\n",
    "\n",
    "# ì²« ê¸€ìë³„ í‰ê·  ê°€ê²© ê³„ì‚°\n",
    "first_letter_mean_price = train_data.groupby('first_letter')['price'].mean().reset_index()\n",
    "first_letter_mean_price.columns = ['first_letter', 'first_letter_mean_price']\n",
    "df = df.merge(first_letter_mean_price, on='first_letter', how='left')\n",
    "print(\"â€˜first_letterâ€™ì— ëŒ€í•´ íƒ€ê²Ÿ ì¸ì½”ë”©(â€˜first_letter_mean_priceâ€™) ì ìš© ì™„ë£Œ.\")\n",
    "\n",
    "# ë§ˆì§€ë§‰ ê¸€ì ì¡°í•©ë³„ í‰ê·  ê°€ê²© ê³„ì‚°\n",
    "last_letters_mean_price = train_data.groupby('last_letters')['price'].mean().reset_index()\n",
    "last_letters_mean_price.columns = ['last_letters', 'last_letters_mean_price']\n",
    "df = df.merge(last_letters_mean_price, on='last_letters', how='left')\n",
    "print(\"â€˜last_lettersâ€™ì— ëŒ€í•´ íƒ€ê²Ÿ ì¸ì½”ë”©(â€˜last_letters_mean_priceâ€™) ì ìš© ì™„ë£Œ.\")\n",
    "\n",
    "# íƒ€ê²Ÿ ë³€ìˆ˜ 'price'ì— ë¡œê·¸ ë³€í™˜ ì ìš©\n",
    "# íšŒê·€ ëª¨ë¸ì—ì„œ íƒ€ê²Ÿ ë¶„í¬ë¥¼ ì •ê·œë¶„í¬ì— ê°€ê¹ê²Œ ë§Œë“¤ê³  ì´ìƒì¹˜ ì˜í–¥ ê°ì†Œ ëª©ì \n",
    "df['log_price'] = np.log1p(df['price'])\n",
    "print(\"â€˜priceâ€™ì— ë¡œê·¸ ë³€í™˜(log1p) ì ìš©í•˜ì—¬ â€˜log_priceâ€™ ìƒì„± ì™„ë£Œ.\")\n",
    "# --- ëª¨ë¸ë§ í–¥ìƒì„ ìœ„í•œ ì‹ ê·œ í”¼ì²˜ ì¶”ê°€ ---\n",
    "\n",
    "# ìˆ«ì ê¸¸ì´ ë° ë‹¨ì¼ ìˆ«ìì¸ì§€ ì—¬ë¶€\n",
    "df['number_length'] = df['numbers'].apply(lambda x: len(str(x))) # ìˆ«ì ë¶€ë¶„ ê¸¸ì´\n",
    "df['is_single_digit'] = (df['number_length'] == 1).astype(int) # í•œ ìë¦¬ ìˆ«ìì¸ì§€ ì—¬ë¶€ (ì´ì§„ í”Œë˜ê·¸)\n",
    "print(\"â€˜number_lengthâ€™ì™€ â€˜is_single_digitâ€™ í”¼ì²˜ ì¶”ê°€ ì™„ë£Œ.\")\n",
    "\n",
    "# ê¸€ì + ì§€ì—­ ì¡°í•© ë¹ˆë„\n",
    "# íŠ¹ì • ì§€ì—­ ë‚´ ì°¨ëŸ‰ ë²ˆí˜¸íŒ íŒ¨í„´ì˜ ì¸ê¸° ë˜ëŠ” í¬ì†Œì„±ì„ ë°˜ì˜\n",
    "df['letters_region'] = df['full_letters'] + \"_\" + df['region_code_original'].astype(str)\n",
    "freq_lr = df['letters_region'].value_counts(normalize=True).to_dict()\n",
    "df['letters_region_freq'] = df['letters_region'].map(freq_lr)\n",
    "print(\"â€˜letters_region_freqâ€™ (ê¸€ì-ì§€ì—­ ì¡°í•© ë¹ˆë„) ê³„ì‚° ì™„ë£Œ.\")\n",
    "\n",
    "# ìƒëŒ€ì  ëª…ì„± ìˆœìœ„ (prestige_scoreë¥¼ 0~1 ì‚¬ì´ë¡œ ì •ê·œí™”í•œ ìˆœìœ„)\n",
    "# ì „ì²´ ë²ˆí˜¸íŒ ì¤‘ ìƒëŒ€ì  ëª…ì„± ì •ë„ë¥¼ ë‚˜íƒ€ëƒ„\n",
    "from scipy.stats import rankdata\n",
    "df['prestige_rank'] = rankdata(df['prestige_score'].astype(int), method='average') / len(df)\n",
    "print(\"â€˜prestige_scoreâ€™ë¥¼ ê¸°ë°˜ìœ¼ë¡œ â€˜prestige_rankâ€™ ìƒì„± ì™„ë£Œ.\")\n",
    "\n",
    "# ìƒí˜¸ì‘ìš© í”¼ì²˜ ìƒì„±\n",
    "df['letter_number_combo'] = df['full_letters'] + \"_\" + df['numbers'].astype(str)\n",
    "# 'is_government'ì™€ 'prestige_score' ê°„ ìƒí˜¸ì‘ìš©\n",
    "df['is_gov_and_prestige'] = df['is_government'] * df['prestige_score'].astype(int)\n",
    "print(\"â€˜letter_number_comboâ€™ ë° â€˜is_gov_and_prestigeâ€™ ìƒí˜¸ì‘ìš© í”¼ì²˜ ì¶”ê°€ ì™„ë£Œ.\")\n",
    "\n",
    "# ì•Œë ¤ì§„ ë²ˆí˜¸íŒê³¼ì˜ ìœ ì‚¬ë„ (CountVectorizerë¥¼ í™œìš©í•œ ë¬¸ì ì„ë² ë”©)\n",
    "# ê¸€ì ì‹œí€€ìŠ¤ íŒ¨í„´ í¬ì°© ì‹œë„\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# ë¬¸ì n-gram (1~2ê¸€ì ì¡°í•©) ì¶”ì¶œ\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(1,2))\n",
    "# 'full_letters' ì»¬ëŸ¼ì— ì ìš© (ì˜ˆ: 'XAA', 'TMM')\n",
    "letter_features = vectorizer.fit_transform(df['full_letters'].fillna(''))\n",
    "# ì°¸ê³ : 'letter_features'ëŠ” í¬ì†Œí–‰ë ¬ë¡œ, ëª¨ë¸ë§ íŒŒì´í”„ë¼ì¸ì— í†µí•©í•˜ì—¬ ì‚¬ìš© ê°€ëŠ¥\n",
    "print(f\"CountVectorizerë¡œ â€˜full_lettersâ€™ ë¬¸ì íŠ¹ì§• ìƒì„± ì™„ë£Œ. í–‰ë ¬ í¬ê¸°: {letter_features.shape}\")\n",
    "\n",
    "# ì„¸ë¶„í™”ëœ ì§€ë¦¬ ì •ë³´\n",
    "# ì£¼ìš” ëŒ€ë„ì‹œ ë° ê²½ì œ ì¤‘ì‹¬ì§€ë¥¼ í”„ë¦¬ë¯¸ì—„ ì§€ì—­ìœ¼ë¡œ í‘œì‹œí•˜ëŠ” ì´ì§„ í”¼ì²˜\n",
    "premium_regions = ['Moscow', 'Saint Petersburg', 'Moscow Oblast']\n",
    "df['is_premium_region'] = df['region_name'].isin(premium_regions).astype(int)\n",
    "print(\"ì£¼ìš” ê²½ì œ ì¤‘ì‹¬ì§€ì— ëŒ€í•´ â€˜is_premium_regionâ€™ í”¼ì²˜ ìƒì„± ì™„ë£Œ.\")\n",
    "\n",
    "# í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ ë©”ì‹œì§€ ì¶œë ¥\n",
    "print(\"\\ní”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ. ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ DataFrame ì¤€ë¹„ ì™„ë£Œ.\")\n",
    "print(f\"í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ í›„ ìµœì¢… DataFrame í¬ê¸°: {df.shape}\")\n",
    "\n",
    "Model Training and Ensemble\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "categorical_features = ['brand', 'model', 'fuel_type', 'transmission']  # ì˜ˆì‹œ\n",
    "\n",
    "model = CatBoostRegressor(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    random_state=SEED,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train, cat_features=categorical_features)\n",
    "\n",
    "ì¸ì½”ë”© ì—†ì´ CatBoost ì‚¬ìš©í•˜ëŠ” ë°©ì‹ ->  CatBoost ì“¸ê±°ë©´ ì´ê±° ì‚¬ìš©\n",
    "df.columns\n",
    "# ---------------------------------------------\n",
    "# âš™ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ì „ì—­ ì„¤ì •\n",
    "# ---------------------------------------------\n",
    "SEED = 92       # ì¬í˜„ì„± í™•ë³´ìš© ëœë¤ ì‹œë“œ\n",
    "N_SPLITS = 10   # Stratified K-Fold êµì°¨ê²€ì¦ìš© ë¶„í•  ìˆ˜\n",
    "TARGET = 'log_price'  # ì˜ˆì¸¡í•  íƒ€ê²Ÿ ë³€ìˆ˜\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ğŸ§¹ í•™ìŠµ ì‹œ ì œì™¸í•  ì»¬ëŸ¼ ëª©ë¡\n",
    "# ---------------------------------------------\n",
    "DROP_COLS = [\n",
    "    'id', 'plate', 'price', 'log_price', 'is_train',\n",
    "    \"is_number_000\", \"is_number_444\", \"is_number_222\", \"is_number_700\", \n",
    "    \"is_number_555\", \"quarter\", \"day_of_week\", \"is_weekend\",\n",
    "    \"prestige_score\", \"is_number_300\",\"is_number_333\",\"is_number_400\",\n",
    "    'significance_level', 'numbers_log_freq_enc', 'is_gov_and_prestige'\n",
    "]\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ğŸ“Š ë²”ì£¼í˜• ë° ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ì§€ì •\n",
    "# ---------------------------------------------\n",
    "categorical_features = [\n",
    "    'brand', 'model', 'fuel_type', 'transmission', 'color', 'region'\n",
    "]\n",
    "\n",
    "numeric_features = [\n",
    "    'engine_size', 'mileage', 'year', 'power', 'doors', 'seats'\n",
    "]\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ğŸ”„ ì „ì²˜ë¦¬ê¸° êµ¬ì„±\n",
    "# ---------------------------------------------\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "categorical_transformer = OrdinalEncoder(\n",
    "    handle_unknown='use_encoded_value',\n",
    "    unknown_value=-1\n",
    ")\n",
    "\n",
    "numeric_transformer = 'passthrough'  # ìˆ˜ì¹˜í˜•ì€ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('cat', categorical_transformer, categorical_features),\n",
    "    ('num', numeric_transformer, numeric_features)\n",
    "])\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ğŸ§± ìµœì¢… íŒŒì´í”„ë¼ì¸ (XGBoost ì‚¬ìš© ì˜ˆì‹œ)\n",
    "# ---------------------------------------------\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', CatBoostRegressor(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    random_state=SEED,\n",
    "    verbose=100\n",
    "    ))\n",
    "])\n",
    "## 1. TRAIN / TEST SPLIT\n",
    "# 'is_train' í”Œë˜ê·¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•©ì³ì¡Œë˜ DataFrameì„ ì›ë˜ì˜ í•™ìŠµìš©ê³¼ í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ë¡œ ë¶„ë¦¬\n",
    "train_df = df[df['is_train'] == 1].copy()  # .copy()ë¥¼ ì‚¬ìš©í•˜ì—¬ SettingWithCopyWarning ë°©ì§€\n",
    "test_df = df[df['is_train'] == 0].copy()\n",
    "\n",
    "# í•™ìŠµ ë°ì´í„°ì…‹ì—ì„œ íŠ¹ì§•(X)ê³¼ íƒ€ê²Ÿ(y)ì„ ì •ì˜í•˜ê³ ,\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì—ì„œëŠ” ì˜ˆì¸¡ì— ì‚¬ìš©í•  íŠ¹ì§•(X_test)ë§Œ ì •ì˜\n",
    "# DROP_COLSì— ëª…ì‹œëœ ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì•„ë„ ì—ëŸ¬ê°€ ë°œìƒí•˜ì§€ ì•Šë„ë¡ errors='ignore' ì‚¬ìš©\n",
    "X = train_df.drop(columns=DROP_COLS, errors='ignore')  # í•™ìŠµìš© íŠ¹ì§•\n",
    "y = train_df[TARGET].copy()                            # íƒ€ê²Ÿê°’\n",
    "X_test = test_df.drop(columns=DROP_COLS, errors='ignore')  # í…ŒìŠ¤íŠ¸ìš© íŠ¹ì§•\n",
    "\n",
    "print(\"ë°ì´í„°ë¥¼ í•™ìŠµìš©ê³¼ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë¶„ë¦¬í–ˆìŠµë‹ˆë‹¤.\")\n",
    "print(f\"í•™ìŠµ íŠ¹ì§•(X) ë°ì´í„° í˜•íƒœ: {X.shape}\")\n",
    "print(f\"í•™ìŠµ íƒ€ê²Ÿ(y) ë°ì´í„° í˜•íƒœ: {y.shape}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ íŠ¹ì§•(X_test) ë°ì´í„° í˜•íƒœ: {X_test.shape}\")\n",
    "X.columns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì¶”ì¶œ\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# ìƒê´€ í–‰ë ¬ ê³„ì‚°\n",
    "corr_matrix = df[numerical_cols].corr().abs()\n",
    "\n",
    "# ìƒì‚¼ê° í–‰ë ¬ë§Œ ë‚¨ê¸°ê¸° (ìê¸° ìì‹  ì œì™¸)\n",
    "upper_tri = corr_matrix.where(~np.tril(np.ones(corr_matrix.shape)).astype(bool))\n",
    "\n",
    "# ìƒê´€ê³„ìˆ˜ 0.85 ì´ìƒì¸ ì»¬ëŸ¼ë“¤ ì¶”ì¶œ\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.85)]\n",
    "\n",
    "print(\"ë‹¤ì¤‘ê³µì„ ì„± ì˜ì‹¬ ì»¬ëŸ¼ë“¤:\\n\", to_drop)\n",
    "\n",
    "\n",
    "## 2. AUTOMATIC COLUMN DETECTION (Enhanced)\n",
    "# ì´ ì„¹ì…˜ì€ ë‹¤ì–‘í•œ ë°ì´í„° íƒ€ì…ì„ ë™ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ë° í•µì‹¬ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
    "# ìˆ«ìí˜•, ë¶ˆë¦¬ì–¸í˜•, ë²”ì£¼í˜• ì»¬ëŸ¼ì„ ìë™ìœ¼ë¡œ ì‹ë³„í•˜ê³ ,\n",
    "# ë²”ì£¼í˜• ì»¬ëŸ¼ì€ ê³ ìœ ê°’ ê°œìˆ˜(ì¹´ë””ë„ë¦¬í‹°)ì— ë”°ë¼ ì„¸ë¶„í™”í•˜ì—¬\n",
    "# ì ì ˆí•œ ì¸ì½”ë”© ì „ëµì„ ì ìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\n",
    "\n",
    "def detect_columns(X):\n",
    "    \"\"\"\n",
    "    ë°ì´í„° íƒ€ì… ë° ì¹´ë””ë„ë¦¬í‹° ê¸°ì¤€ìœ¼ë¡œ ì»¬ëŸ¼ë“¤ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤.\n",
    "    ê° ì»¬ëŸ¼ ìœ í˜•ì— ë§ëŠ” ì „ì²˜ë¦¬ ë‹¨ê³„ë¥¼ ì ìš©í•˜ëŠ” ë° í™œìš©ë©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    bool_cols = [c for c in X.columns if X[c].dtype == 'bool']  # ë¶ˆë¦¬ì–¸(boolean) ì»¬ëŸ¼ ì‹ë³„\n",
    "    num_cols = [c for c in X.columns if X[c].dtype.kind in 'if' and c not in bool_cols]  # ìˆ«ìí˜•(int/float) ì»¬ëŸ¼ ì‹ë³„ (ë¶ˆë¦¬ì–¸ ì œì™¸)\n",
    "    cat_cols = [c for c in X.columns if c not in num_cols + bool_cols]  # ë‚˜ë¨¸ì§€ëŠ” ë²”ì£¼í˜•(categorical) ì»¬ëŸ¼ìœ¼ë¡œ ê°„ì£¼\n",
    "\n",
    "    # ë²”ì£¼í˜• ì»¬ëŸ¼ì„ ê³ ìœ ê°’ ê°œìˆ˜(ì¹´ë””ë„ë¦¬í‹°)ì— ë”°ë¼ ì„¸ë¶„í™”\n",
    "    # ì¹´ë””ë„ë¦¬í‹°ì— ë”°ë¼ ìµœì ì˜ ì¸ì½”ë”© ë°©ì‹ì´ ë‹¬ë¼ì§€ë¯€ë¡œ ì´ë¥¼ ê³ ë ¤í•¨\n",
    "    cat_low = [c for c in cat_cols if X[c].nunique() <= 20]      # ê³ ìœ ê°’ì´ 20 ì´í•˜ â†’ One-Hot Encoding ì í•©\n",
    "    cat_mid = [c for c in cat_cols if 20 < X[c].nunique() <= 200] # ê³ ìœ ê°’ì´ ì¤‘ê°„ ìˆ˜ì¤€ â†’ Ordinal Encoding ì í•©\n",
    "    cat_high = [c for c in cat_cols if X[c].nunique() > 200]     # ê³ ìœ ê°’ì´ ë§¤ìš° ë§ìŒ â†’ Target Encoding ì í•©\n",
    "\n",
    "    print('\\nì»¬ëŸ¼ ìš”ì•½ âœ ìˆ«ìí˜•:', len(num_cols),\n",
    "          '| ë¶ˆë¦¬ì–¸:', len(bool_cols),\n",
    "          '| ë‚®ì€ ì¹´ë””ë„ë¦¬í‹° ë²”ì£¼í˜•:', len(cat_low),\n",
    "          '| ì¤‘ê°„ ì¹´ë””ë„ë¦¬í‹° ë²”ì£¼í˜•:', len(cat_mid),\n",
    "          '| ë†’ì€ ì¹´ë””ë„ë¦¬í‹° ë²”ì£¼í˜•:', len(cat_high))\n",
    "\n",
    "    return num_cols, bool_cols, cat_low, cat_mid, cat_high\n",
    "\n",
    "# í•™ìŠµ ë°ì´í„°ì˜ í”¼ì²˜(X)ì— ëŒ€í•´ ì»¬ëŸ¼ ë¶„ë¥˜ í•¨ìˆ˜ ì ìš©\n",
    "num_cols, bool_cols, cat_low, cat_mid, cat_high = detect_columns(X)\n",
    "## 3. PREPROCESSING PIPELINE (Comprehensive and Flexible)\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# ë‹¤ì–‘í•œ ë°ì´í„° íƒ€ì…ì— ë§ì¶° ì ì ˆí•œ ì „ì²˜ë¦¬ ë°©ì‹ì„ ë³‘ë ¬ë¡œ ì ìš©í•  ìˆ˜ ìˆë„ë¡ í•´ì£¼ëŠ” ColumnTransformerë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "# ì´ êµ¬ì„±ì€ ëª¨ë¸ í•™ìŠµ ì „ì— ê° íŠ¹ì„±ì— ë§ëŠ” ì „ì²˜ë¦¬ë¥¼ ì•ˆì •ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë“¤ì€ íŠ¹ë³„í•œ ë³€í™˜ ì—†ì´ ê·¸ëŒ€ë¡œ ì‚¬ìš© ('passthrough'ëŠ” ì•„ë¬´ ë³€í™˜ë„ í•˜ì§€ ì•ŠìŒì„ ì˜ë¯¸)\n",
    "        ('num', 'passthrough', num_cols),\n",
    "        \n",
    "        # ì´ì§„ ë³€ìˆ˜ (True/False)ëŠ” ì´ë¯¸ 0ê³¼ 1ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "        ('bool', 'passthrough', bool_cols),\n",
    "\n",
    "        # ì €ì¤‘ë³µ ë²”ì£¼í˜• ë³€ìˆ˜: One-Hot Encoding ì‚¬ìš©\n",
    "        # - ê° ë²”ì£¼ë§ˆë‹¤ ìƒˆë¡œìš´ ì—´ì„ ìƒì„±í•˜ì—¬ 0 ë˜ëŠ” 1ì˜ ê°’ìœ¼ë¡œ í‘œí˜„\n",
    "        # - handle_unknown='ignore' ì„¤ì •ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ì…‹ì—ì„œ ìƒˆë¡œìš´ ë²”ì£¼ê°€ ë“±ì¥í•´ë„ ì˜¤ë¥˜ ì—†ì´ ì²˜ë¦¬\n",
    "        # - sparse_output=FalseëŠ” ë°€ì§‘ ë°°ì—´ë¡œ ì¶œë ¥í•˜ê²Œ í•˜ì—¬ ì´í›„ pandasë‚˜ numpyì™€ì˜ ì—°ë™ì„ ì‰½ê²Œ í•¨\n",
    "        ('low', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_low),\n",
    "\n",
    "        # ì¤‘ë³µ ìˆ˜ê°€ ì¤‘ê°„ ìˆ˜ì¤€ì¸ ë²”ì£¼í˜• ë³€ìˆ˜: Ordinal Encoding ì‚¬ìš©\n",
    "        # - ê° ë²”ì£¼ì— ê³ ìœ  ì •ìˆ˜ê°’ì„ ë§¤ê²¨ì„œ ìˆ«ìë¡œ ë³€í™˜\n",
    "        # - handle_unknown='use_encoded_value', unknown_value=-1 ì˜µì…˜ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë²”ì£¼ê°€ ë‚˜ì™€ë„ ì˜ˆì™¸ ë°œìƒ ë°©ì§€\n",
    "        ('mid', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), cat_mid),\n",
    "\n",
    "        # ê³ ì¤‘ë³µ ë²”ì£¼í˜• ë³€ìˆ˜: ì›ë˜ëŠ” Target Encodingì„ ì‚¬ìš©í–ˆìœ¼ë‚˜, ì™¸ë¶€ íŒ¨í‚¤ì§€ ì˜ì¡´ì„± ì œê±°ë¥¼ ìœ„í•´ Ordinal Encodingìœ¼ë¡œ ëŒ€ì²´\n",
    "        # - ë†’ì€ ì¹´ë””ë„ë¦¬í‹°(ë²”ì£¼ì˜ ìˆ˜)ê°€ ìˆë”ë¼ë„, ê° ë²”ì£¼ë¥¼ ê³ ìœ  ì •ìˆ˜ë¡œ ë§¤í•‘í•˜ì—¬ ì²˜ë¦¬\n",
    "        # - unknown_value=-1 ì„¤ì •ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ìƒˆë¡œìš´ ë²”ì£¼ê°€ ë‚˜ì™€ë„ ì•ˆì •ì ìœ¼ë¡œ ì²˜ë¦¬ ê°€ëŠ¥\n",
    "        ('high', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), cat_high)\n",
    "    ],\n",
    "\n",
    "    # ìœ„ì—ì„œ ì§€ì •í•˜ì§€ ì•Šì€ ì—´ë“¤ì€ ëª¨ë‘ ì‚­ì œ (ë¶ˆí•„ìš”í•œ ì •ë³´ê°€ ëª¨ë¸ì— ë“¤ì–´ê°€ì§€ ì•Šë„ë¡ ì•ˆì „ ì¡°ì¹˜)\n",
    "    remainder='drop',\n",
    "\n",
    "    # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì†ë„ í–¥ìƒì„ ìœ„í•´ CPU ëª¨ë“  ì½”ì–´ë¥¼ ì‚¬ìš©\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nPreprocessing pipeline (ColumnTransformer) defined.\")\n",
    "## 4. MODELS AND PARAMETERS (Optimized and Modular)\n",
    "# ë‹¤ì–‘í•œ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… íšŒê·€ ëª¨ë¸ì— ëŒ€í•´ ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "# ì´ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ì€ GridSearchCV, RandomizedSearchCV, ë˜ëŠ” Optunaì™€ ê°™ì€ ê³ ê¸‰ ìµœì í™” ê¸°ë²•ì„ í†µí•´ ë„ì¶œë¨\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "xgb_params = {\n",
    "    'n_estimators': 1433,  # íŠ¸ë¦¬ ê°œìˆ˜\n",
    "    'max_depth': 12,  # íŠ¸ë¦¬ ìµœëŒ€ ê¹Šì´\n",
    "    'learning_rate': 0.01852160907217988,  # í•™ìŠµë¥ \n",
    "    'subsample': 0.6786672470738663,  # ìƒ˜í”Œë§ ë¹„ìœ¨ (row)\n",
    "    'colsample_bytree': 0.46208650739218005,  # í”¼ì²˜ ìƒ˜í”Œë§ ë¹„ìœ¨ (column)\n",
    "    'reg_alpha': 0.017519138973638618,  # L1 ì •ê·œí™”\n",
    "    'reg_lambda': 0.2839310763317462,  # L2 ì •ê·œí™”\n",
    "    'gamma': 0.0033995958574628547,  # ë¶„í•  ìµœì†Œ ì†ì‹¤ ê°ì†Œ ê°’\n",
    "    'tweedie_variance_power': 1.0869464555654937,  # Tweedie ë¶„í¬ íŒŒë¼ë¯¸í„° (ë¹„ëŒ€ì¹­ íƒ€ê¹ƒì— ìœ ìš©)\n",
    "    'objective': 'reg:tweedie',  # íšŒê·€ ëª©ì  í•¨ìˆ˜\n",
    "    'n_jobs': -1,  # ë©€í‹°ìŠ¤ë ˆë”© ì‚¬ìš©\n",
    "    'random_state': SEED  # ëœë¤ ì‹œë“œ ê³ ì •\n",
    "}\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "lgb_params = {\n",
    "    'n_estimators': 999,  # ë¶€ìŠ¤íŒ… round ìˆ˜ (íŠ¸ë¦¬ ê°œìˆ˜)\n",
    "    'max_depth': 11,  # íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´\n",
    "    'learning_rate': 0.07607568555547708,  # í•™ìŠµë¥ \n",
    "    'subsample': 0.6363036032688429,  # í•™ìŠµ ìƒ˜í”Œì˜ ë¹„ìœ¨ (row-wise sampling)\n",
    "    'colsample_bytree': 0.5072021102992719,  # ê° íŠ¸ë¦¬ì—ì„œ ì‚¬ìš©í•  í”¼ì²˜ì˜ ë¹„ìœ¨ (column-wise sampling)\n",
    "    'min_child_samples': 97,  # ë¦¬í”„ ë…¸ë“œê°€ ê°€ì ¸ì•¼ í•˜ëŠ” ìµœì†Œ ë°ì´í„° ìˆ˜\n",
    "    'reg_alpha': 0.16671454380081874,  # L1 ì •ê·œí™” íŒŒë¼ë¯¸í„°\n",
    "    'reg_lambda': 0.6455320711051608,  # L2 ì •ê·œí™” íŒŒë¼ë¯¸í„°\n",
    "    'n_jobs': -1,  # ëª¨ë“  CPU ì½”ì–´ ì‚¬ìš©\n",
    "    'random_state': SEED  # ëœë¤ ì‹œë“œ ê³ ì •\n",
    "}\n",
    "\n",
    "# CatBoost í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "cat_params = {\n",
    "    'iterations': 991,  # íŠ¸ë¦¬ ê°œìˆ˜\n",
    "    'depth': 10,  # íŠ¸ë¦¬ ê¹Šì´\n",
    "    'learning_rate': 0.06462213707942074,  # í•™ìŠµë¥ \n",
    "    'l2_leaf_reg': 1.9289204888270515,  # L2 ì •ê·œí™” (ë¦¬í”„ë³„)\n",
    "    'subsample': 0.7213225292844163,  # ë°ì´í„° ìƒ˜í”Œë§ ë¹„ìœ¨\n",
    "    'bagging_temperature': 0.4361642090192932,  # ìƒ˜í”Œ ë‹¤ì–‘ì„± ì¡°ì ˆ\n",
    "    'random_strength': 6.443179917768372,  # ë¶„í•  ì‹œ ëœë¤ì„± ì¡°ì ˆ\n",
    "    'min_data_in_leaf': 72,  # ë¦¬í”„ ìµœì†Œ ìƒ˜í”Œ ìˆ˜\n",
    "    'loss_function': 'RMSE',  # ì†ì‹¤ í•¨ìˆ˜ (íšŒê·€ì— ì¼ë°˜ì ì¸ RMSE)\n",
    "    'verbose': 0,  # í›ˆë ¨ ë¡œê·¸ ìƒëµ\n",
    "    'random_state': SEED\n",
    "}\n",
    "\n",
    "# ì‚¬ìš©í•  ëª¨ë¸ë“¤ì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì •ì˜ (ì¶”ê°€ ëª¨ë¸ ì‰½ê²Œ í¬í•¨ ê°€ëŠ¥)\n",
    "models = {\n",
    "    'XGB': XGBRegressor(**xgb_params),\n",
    "    'LGBM': LGBMRegressor(**lgb_params),\n",
    "    'CatBoost': CatBoostRegressor(**cat_params)\n",
    "}\n",
    "print(\"\\nëª¨ë¸ ë° ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜ ì™„ë£Œ.\")\n",
    "\n",
    "# ê° ëª¨ë¸ë³„ íŒŒì´í”„ë¼ì¸ êµ¬ì„±: ì „ì²˜ë¦¬(preprocessing) â†’ ëª¨ë¸ í•™ìŠµ\n",
    "# ê° íŒŒì´í”„ë¼ì¸ì€ í•™ìŠµ ì „ì— í•„ìš”í•œ ë°ì´í„° ì „ì²˜ë¦¬ê¹Œì§€ í¬í•¨\n",
    "pipelines = {name: Pipeline(steps=[('prep', preprocess), ('model', model)]) for name, model in models.items()}\n",
    "print(\"íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì™„ë£Œ: ì „ì²˜ë¦¬ -> ëª¨ë¸.\")\n",
    "\n",
    "## 5. SMAPE METRIC Definition\n",
    "# SMAPE(Symmetric Mean Absolute Percentage Error)ëŠ” ì‹œê³„ì—´ ì˜ˆì¸¡ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” ì§€í‘œì´ë©°,\n",
    "# ì‹¤ì œê°’ì´ 0ì¸ ê²½ìš°ì—ë„ ë¹„êµì  ê°•ê±´í•œ í‰ê°€ ì§€í‘œì…ë‹ˆë‹¤.\n",
    "# í•œ ë²ˆ ì •ì˜í•´ë‘ë©´ ì¼ê´€ëœ í‰ê°€ë¥¼ ìœ„í•œ ê¸°ì¤€ì´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    SMAPE(Symmetric Mean Absolute Percentage Error)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "    ê³µì‹: (1/n) * Î£(|y_true - y_pred| / ((|y_true| + |y_pred|) / 2)) * 100\n",
    "    ì´ ì§€í‘œëŠ” y_trueë‚˜ y_predê°€ 0ì¼ ìˆ˜ ìˆëŠ” ìƒí™©ì—ì„œë„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    # ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ì ˆëŒ“ê°’ í‰ê· ì„ ë¶„ëª¨ë¡œ ì‚¬ìš©\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    # ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ ì°¨ì´ì˜ ì ˆëŒ“ê°’ (ë¶„ì)\n",
    "    diff = np.abs(y_true - y_pred)\n",
    "    \n",
    "    # ë¶„ëª¨ê°€ 0ì¸ ê²½ìš°ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ë°°ì—´ ì´ˆê¸°í™”\n",
    "    # ë¶„ëª¨ê°€ 0ì¸ ê²½ìš° í•´ë‹¹ í•­ì˜ SMAPEëŠ” 0ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ NaN ë˜ëŠ” Inf ë°©ì§€\n",
    "    smape_term = np.zeros_like(diff, dtype=float)\n",
    "    \n",
    "    # ë¶„ëª¨ê°€ 0ì´ ì•„ë‹Œ ì¸ë±ìŠ¤ë§Œ ì„ íƒí•˜ì—¬ ë‚˜ëˆ—ì…ˆ ìˆ˜í–‰\n",
    "    non_zero_denom = denominator != 0\n",
    "    smape_term[non_zero_denom] = diff[non_zero_denom] / denominator[non_zero_denom]\n",
    "    \n",
    "    # ì „ì²´ í‰ê· ì„ ë‚´ì–´ ë°±ë¶„ìœ¨ë¡œ ë°˜í™˜\n",
    "    return np.mean(smape_term) * 100\n",
    "\n",
    "print(\"\\nSMAPE í‰ê°€ ì§€í‘œê°€ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "## 6. STRATIFIED CROSS-VALIDATION (on the target)\n",
    "# Stratified K-Fold êµì°¨ê²€ì¦ì€ ê° í´ë“œì— íƒ€ê²Ÿ ë³€ìˆ˜ì˜ ë¶„í¬ê°€ ê³ ë¥´ê²Œ ë‚˜íƒ€ë‚˜ë„ë¡ í•´ì¤ë‹ˆë‹¤.\n",
    "# ì´ëŠ” íƒ€ê²Ÿì´ ë¶ˆê· í˜•í•˜ê±°ë‚˜ íŠ¹ì • êµ¬ê°„ì´ ì¤‘ìš”í•  ë•Œ íŠ¹íˆ ìœ ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "# íƒ€ê²Ÿ ë³€ìˆ˜(log_price)ë¥¼ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ  ê³„ì¸µí™”(Stratification)ë¥¼ ìœ„í•œ \"ì¸µ(strata)\" ìƒì„±\n",
    "# íšŒê·€ ë¬¸ì œë¥¼ ë¶„ë¥˜ ë¬¸ì œì²˜ëŸ¼ ì·¨ê¸‰í•˜ì—¬ K-Fold ë¶„í•  ì‹œ íƒ€ê²Ÿ ë¶„í¬ë¥¼ ìœ ì§€í•¨\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "y_bins = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile') \\\n",
    "    .fit_transform(y.values.reshape(-1, 1)).astype(int).ravel()\n",
    "print(f\"\\níƒ€ê²Ÿ ë³€ìˆ˜ ('{TARGET}')ê°€ {y_bins.max() + 1}ê°œì˜ ì¸µìœ¼ë¡œ êµ¬ê°„í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# StratifiedKFold ê°ì²´ ì´ˆê¸°í™” (shuffle=Trueë¡œ ë¬´ì‘ìœ„ ì…”í”Œ, random_state ê³ ì •)\n",
    "kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# ê° ëª¨ë¸ì˜ OOF(out-of-fold) ì˜ˆì¸¡ê°’ê³¼ í…ŒìŠ¤íŠ¸ì…‹ ì˜ˆì¸¡ê°’ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”\n",
    "# OOFëŠ” ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ê³„ì‚° ë° ìµœì¢… CV í‰ê°€ì— ì‚¬ìš©\n",
    "# í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ê°’ì€ í´ë“œë³„ ì˜ˆì¸¡ì„ í‰ê· ë‚´ì–´ ì œì¶œìš©ìœ¼ë¡œ ì‚¬ìš©\n",
    "oof_preds = {name: np.zeros(len(y)) for name in models}\n",
    "test_preds = {name: np.zeros(len(X_test)) for name in models}\n",
    "feature_importances = {}  # ëª¨ë¸ë³„ ì¤‘ìš” í”¼ì²˜ ì €ì¥ìš©\n",
    "\n",
    "print('\\n===== êµì°¨ê²€ì¦ ê¸°ë°˜ í›ˆë ¨ ì‹œì‘ =====')\n",
    "\n",
    "# ê° ëª¨ë¸ì— ëŒ€í•´ êµì°¨ê²€ì¦ì„ ìˆ˜í–‰\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    print(f\"\\nëª¨ë¸ í›ˆë ¨ ì‹œì‘: {model_name}...\")\n",
    "    try:\n",
    "        # StratifiedKFoldì—ì„œ ê° í´ë“œì— ëŒ€í•´ ë°˜ë³µ\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X, y_bins), 1):\n",
    "            print(f\"  í´ë“œ {fold:02d}/{N_SPLITS}\")\n",
    "            \n",
    "            # í˜„ì¬ í´ë“œì˜ í•™ìŠµ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„° ë¶„ë¦¬\n",
    "            X_tr, y_tr = X.iloc[train_idx], y.iloc[train_idx]\n",
    "            X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "            # íŒŒì´í”„ë¼ì¸ í•™ìŠµ (ì „ì²˜ë¦¬ + ëª¨ë¸ í•™ìŠµ í¬í•¨)\n",
    "            pipeline.fit(X_tr, y_tr)\n",
    "\n",
    "            # í˜„ì¬ í´ë“œì˜ ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ê°’ ì €ì¥ (OOF ì˜ˆì¸¡)\n",
    "            oof_preds[model_name][val_idx] = pipeline.predict(X_val)\n",
    "            \n",
    "            # í…ŒìŠ¤íŠ¸ì…‹ ì˜ˆì¸¡ê°’ ëˆ„ì  (í´ë“œë³„ í‰ê·  ë‚´ì–´ ì‚¬ìš©)\n",
    "            test_preds[model_name] += pipeline.predict(X_test) / N_SPLITS\n",
    "\n",
    "        # ì „ì²´ í´ë“œì— ëŒ€í•œ OOF ì˜ˆì¸¡ì„ ë°”íƒ•ìœ¼ë¡œ SMAPE ê³„ì‚° (ë¡œê·¸ ìŠ¤ì¼€ì¼ ë³µì› í›„)\n",
    "        cv_smape = smape(np.exp(y), np.exp(oof_preds[model_name]))\n",
    "        print(f'â®•  {model_name} ëª¨ë¸ì˜ ì „ì²´ CV SMAPE: {cv_smape:.2f}%')\n",
    "\n",
    "        # í”¼ì²˜ ì¤‘ìš”ë„ ì¶”ì¶œ (ì§€ì›í•˜ëŠ” ëª¨ë¸ì— í•œí•´)\n",
    "        if hasattr(pipeline['model'], 'feature_importances_'):\n",
    "            # LightGBM, scikit-learnì˜ íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ ë“±\n",
    "            feature_importances[model_name] = pipeline['model'].feature_importances_\n",
    "\n",
    "        elif hasattr(pipeline['model'], 'get_booster'):\n",
    "            # XGBoostëŠ” get_booster()ë¡œ ë‚´ë¶€ Booster ì ‘ê·¼ ê°€ëŠ¥\n",
    "            feature_importances[model_name] = pipeline['model'].get_booster().get_score(importance_type='weight')\n",
    "            # importance_typeì€ 'weight', 'gain', 'cover' ë“±ìœ¼ë¡œ ì§€ì • ê°€ëŠ¥\n",
    "\n",
    "        else:\n",
    "            # ì¤‘ìš”ë„ ì¶”ì¶œ ë¶ˆê°€í•œ ëª¨ë¸ ì²˜ë¦¬\n",
    "            feature_importances[model_name] = None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{model_name} ëª¨ë¸ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "        continue  # ì˜¤ë¥˜ ë°œìƒ ì‹œ í•´ë‹¹ ëª¨ë¸ ê±´ë„ˆë›°ê³  ë‹¤ìŒ ëª¨ë¸ë¡œ\n",
    "## 7. ENSEMBLE PREDICTIONS (Inverse Error Weighting)\n",
    "# 1ì°¨ ëª¨ë¸ë“¤ OOF ì˜ˆì¸¡ê³¼ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ì„ ëª¨ì•„ì„œ ë©”íƒ€ í•™ìŠµìš© ë°ì´í„°ì…‹ ìƒì„±\n",
    "X_meta = np.column_stack([oof_preds[name] for name in models])\n",
    "X_test_meta = np.column_stack([test_preds[name] for name in models])\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "meta_oof = np.zeros(len(y))\n",
    "meta_test_fold = []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_meta):\n",
    "    X_tr, X_val = X_meta[train_idx], X_meta[val_idx]\n",
    "    y_tr, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    meta_model = RidgeCV(alphas=np.logspace(-3, 3, 10), cv=3)\n",
    "    meta_model.fit(X_tr, y_tr)\n",
    "    \n",
    "    meta_oof[val_idx] = meta_model.predict(X_val)\n",
    "    meta_test_fold.append(meta_model.predict(X_test_meta))\n",
    "\n",
    "# í‰ê· ì„ ì·¨í•´ ìµœì¢… í…ŒìŠ¤íŠ¸ì…‹ ì˜ˆì¸¡\n",
    "ensemble_preds_log = np.mean(meta_test_fold, axis=0)\n",
    "ensemble_preds = np.exp(ensemble_preds_log)\n",
    "\n",
    "# OOF ê¸°ì¤€ SMAPE ì¸¡ì •\n",
    "ensemble_smape = smape(np.exp(y), np.exp(meta_oof))\n",
    "print(f\"\\nâ®• K-Fold Stacking Ensemble CV SMAPE: {ensemble_smape:.2f}%\")\n",
    "## 8. SUBMISSION FILE CREATION\n",
    "# ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ë¥¼ Kaggle ì œì¶œìš©ìœ¼ë¡œ ì¤€ë¹„í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.\n",
    "\n",
    "# ëª¨ë¸ ì˜ˆì¸¡ê°’ì— í´ë¦¬í•‘(clipping)ì„ ì ìš©í•˜ì—¬ ë¹„í˜„ì‹¤ì ì¸ ê°’ì„ ë°©ì§€í•©ë‹ˆë‹¤.\n",
    "# a_min=0ì€ ì˜ˆì¸¡ê°’ì´ 0ë³´ë‹¤ ì‘ì•„ì§€ì§€ ì•Šë„ë¡ í•˜ê³ ,\n",
    "# a_max=Noneì€ ìƒí•œ ì œí•œì„ ë‘ì§€ ì•ŠìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "# í´ë¦¬í•‘ì€ ì´ìƒì¹˜ë‚˜ ëª¨ë¸ ì˜¤ë¥˜ë¡œ ì¸í•œ ê·¹ë‹¨ê°’ì„ ì™„í™”í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.\n",
    "ensemble_preds = np.clip(ensemble_preds, a_min=0, a_max=None)\n",
    "\n",
    "# ì œì¶œ íŒŒì¼ì„ ìœ„í•œ DataFrame ìƒì„±: 'id'ì™€ ìµœì¢… 'price' ì˜ˆì¸¡ê°’ì„ í¬í•¨í•©ë‹ˆë‹¤.\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],  # ì›ë³¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ 'id' ì—´ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "    'price': ensemble_preds\n",
    "})\n",
    "\n",
    "# ì œì¶œìš© CSV íŒŒì¼ë¡œ ì €ì¥ (Kaggle í˜•ì‹ì— ë§ê²Œ ì¸ë±ìŠ¤ ì œì™¸)\n",
    "submission.to_csv('0521submission_sample5.csv', index=False)\n",
    "print('\\nâœ…  ì œì¶œ íŒŒì¼ \"submission.csv\" ì´(ê°€) ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.')\n"
   ]
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
